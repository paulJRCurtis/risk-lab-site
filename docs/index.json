[{"uri":"http://paulJRCurtis.github.io/risk-lab-site/1-start-workshop/","title":"Start the Workshop","tags":[],"description":"","content":"To start the workshop, follow one of the following depending on whether you are;\nrunning the workshop on your own (in your own account),\nor\nattending an AWS hosted event (using AWS provided hashes)\n"},{"uri":"http://paulJRCurtis.github.io/risk-lab-site/tags/artifact/","title":"AWS Artifact","tags":[],"description":"&lt;p&gt;AWS Artifact is your go-to, central resource for compliance-related information that matters to you. It provides on-demand access to AWS’ security and compliance reports and select online agreements.&lt;/p&gt;&lt;p&gt;To find out more go to &lt;a href=&#39;https://aws.amazon.com/artifact/&#39; target=&#39;_blank&#39;&gt;AWS Artifact&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;You can access AWS Artifact directly from the &lt;a href=&#39;https://console.aws.amazon.com/artifact&#39; target=&#39;_blank&#39;&gt;AWS Management Console&lt;/a&gt;.&lt;/p&gt;","content":""},{"uri":"http://paulJRCurtis.github.io/risk-lab-site/tags/beginner/","title":"Beginner","tags":[],"description":"blah blah","content":""},{"uri":"http://paulJRCurtis.github.io/risk-lab-site/2-know-the-console/","title":"Getting to Know the Console","tags":[],"description":"","content":"Login to your AWS Account If you haven\u0026rsquo;t already, logon to your AWS account. If you don\u0026rsquo;t have an account, go to Start Workshop.\nLet\u0026rsquo;s get to know the Management Console\nThe AWS Management Console is a web application that comprises and refers to a broad collection of service consoles for managing Amazon Web Services. When you first sign in, you see the console home page.\nAlong the top menubar you will see a few important elements we will use often.\nAWS Logo Clicking on the AWS logo will bring you back to this console screen.\nServices Dropdown Choose Services to open a full list of services. On the upper right of the page, choose Group to see the services listed by category or choose A–Z to see an alphabetical listing. Then choose the service that you want.\nResource Groups Dropdown The AWS Management Console includes the Resource Groups menu, a feature for managing AWS resources such as an Amazon EC2 instance or an Amazon S3 bucket as a group. You can also use the Resource Groups menu to start Tag Editor, a tool for managing and applying labels, or tags, to organize your resources.\nFor more information on creating resource groups and using Tag Editor, see the following topics in the AWS Resource Groups and Tag Editor User Guide.\nPushpin You can add and delete shortcuts for the services that you use most.\nTo add a shortcut choose the pushpin icon and drag a service from the menu to the navigation bar. You can add more shortcuts and drop them onto the navigation bar in any order that you want.\nTo remove a shortcut choose the pushpin icon and drag the shortcut off the navigation bar\nAlert Clicking on alerts will display a dropdown showing Open issues, scheduled changes and Other notifications, as well as the option to show all alerts.\nClicking any of these will take you to the Personal Health Dashboard. Personal Health Dashboard gives you a personalized view into the performance and availability of the AWS services underlying your AWS resources, to learn more go to AWS Personal Health Dashboard.\nUser/Account Dropdown The User/Account Dropdown is on which we will use on occasionally during the labs. Lets step through the information on the dropdown. User - the image below shows the Federated Login that you will see if your identity is provided to the AWS Console form an external identity provider - in the example show this is the AWS Event Engine. If you signed on directly to the console you will see IAM User Account: this is the account Id, the example shows a number but you can also setup and account alias which is easier to remember. The next grouping of options may vary slightly depending on your access but they deal with billing and your credentials. The final option is to Sign Out. Region Dropdown For many services, you can choose a Region that specifies where your resources are managed. You do not choose a Region for the AWS Management Console or for some services, such as IAM. To choose a Region\n In the AWS Management Console, choose a service to go to that service\u0026rsquo;s console. On the navigation bar, choose the name of the currently displayed Region. When you choose a Region, that Region becomes the default in the console.  If you have created AWS resources, but you don’t see those resources in the console, the console might be displaying resources from a different Region. Some resources (such as EC2 instances) are created in a specific Region. To see them, use the Region selector to choose the Region in which the resources were created.\n Support Dropdown This dropdown lists the various support options and resources available to you.\n"},{"uri":"http://paulJRCurtis.github.io/risk-lab-site/1-start-workshop/on-your-own/create-an-aws-account/","title":"Create an AWS Account","tags":[],"description":"","content":" Your account must have the ability to create new IAM roles and scope other IAM permissions.\n   If you don\u0026rsquo;t already have an AWS account with Administrator access: create one now by clicking here\n  Once you have an AWS account, ensure you are following the remaining workshop steps as an IAM user with administrator access to the AWS account: Create a new IAM user to use for the workshop\n  Enter the user details:   Attach the AdministratorAccess IAM Policy:   Click to create the new user:   Take note of the login URL and save:   "},{"uri":"http://paulJRCurtis.github.io/risk-lab-site/1-start-workshop/at-an-aws-event/aws-workshop-portal/","title":"AWS Workshop Portal","tags":[],"description":"","content":"Login to AWS Workshop Portal This workshop creates an AWS account. You will need the Participant Hash provided upon entry, and your email address to track your unique session.\nUse Chrome or Firefox to ensure a good experience.\n Connect to the portal by clicking the button or browsing to https://dashboard.eventengine.run. The following screen will be displayed.\nEnter the provided hash in the text box. The button on the bottom right corner changes to Accept Terms \u0026amp; Login. Click on that button to continue.\nClick on AWS Console on dashboard.\nTake the defaults and click on Open AWS Console. This will open AWS Console in a new browser tab.\n"},{"uri":"http://paulJRCurtis.github.io/risk-lab-site/1-start-workshop/on-your-own/","title":"In your own account","tags":[],"description":"","content":" Only complete this section if you are running the workshop on your own. If you are at an AWS hosted event (such as an Immersion Day, etc), go to At an AWS Event\n Contents  Create an AWS Account   "},{"uri":"http://paulJRCurtis.github.io/risk-lab-site/3-aws-compliance-reports/","title":"Accessing AWS Compliance Reports","tags":["Security of the Cloud","Artifact"],"description":"","content":"In this lab you will access AWS Artifact and download a compliance report and work through guidance on how to interpret the report.\nThis lab is a great place to start if you are new to AWS and mixes a bit of hand on with supporting theory.\nAWS Artifact is your go-to, central resource for compliance-related information that matters to you. It provides on-demand access to AWS’ security and compliance reports and select online agreements.\nContents  Access a report   Interpreting the results   Optional: Shared Responsibility   "},{"uri":"http://paulJRCurtis.github.io/risk-lab-site/4-detective-controls-config/","title":"Detective Controls with Config","tags":["AWS Config","Detective Controls"],"description":"","content":"In this lab we will use three AWS Config rules to demonstrate how to check the configuration of an Amazon Simple Storage Service (Amazon S3) bucket.\nAWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. Config continuously monitors and records your AWS resource configurations and allows you to automate the evaluation of recorded configurations against desired configurations.\nWith Config, you can review changes in configurations and relationships between AWS resources, dive into detailed resource configuration histories, and determine your overall compliance against the configurations specified in your internal guidelines. This enables you to simplify compliance auditing, security analysis, change management, and operational troubleshooting.\nTake a minute (1:34) to watch the video below.\n  "},{"uri":"http://paulJRCurtis.github.io/risk-lab-site/1-start-workshop/at-an-aws-event/","title":"At an AWS Event","tags":[],"description":"","content":" Only complete this section if you are at an AWS hosted event (such as an Immersion Day, or any other event hosted by an AWS employee). If you are running the workshop on your own, go to: On your own.\n Contents  AWS Workshop Portal   "},{"uri":"http://paulJRCurtis.github.io/risk-lab-site/","title":"Risk &amp; Compliance Labs","tags":[],"description":"","content":"Risk \u0026amp; Compliance Labs Introduction These labs have been developed specifically with risk, compliance and controls assurance stakeholders in mind. AWS provides a wealth of services and tools to assist in effective management and governance and provides an unprecedented level of transparency. These labs aim to demonstrate how these services and the telemetry available on the platform can be used to automate controls assurance and provide a real time risk data. These labs have been designed to cater for the absolute beginner, building up to more advanced topics.\nThis repository contains documentation and code in the format of hands-on labs to help you learn, measure, and build. Some labs, particularly the introductory labs maybe be simplified to demonstrate concepts rather than attempting to be production ready approaches. Where this is the case it will be noted in the lab.\nThe labs are categorized into Beginner, Intermediate and Advanced.\nPrerequisites An AWS account that you are able to use for learning and experimenting, that is not used for production or other purposes. If you are doing these labs at an AWS event you may be given an account.\nBeginner Labs For Beginner labs there are no prerequisites and no assumed AWS knowledge, however the labs to build on each other and should ideally be taken in sequence.\nIntermediate Labs For Intermediate and Advanced Labs you may require the following.\n A working directory where you have the rights to create files and directories. A text editor, we will be editing a few configuration files and scripts but not doing any major coding so a basic editor will do - notepad or TextEdit are fine. We will be using zip files so you will need to be able to un-zip and zip files.  "},{"uri":"http://paulJRCurtis.github.io/risk-lab-site/3-aws-compliance-reports/1-access-a-report/","title":"Access a report","tags":[],"description":"","content":"1. Access the AWS Artifact Console From the Console select the Services dropdown and Artifact under Security, Identity, \u0026amp; Compliance.\nClick View reports\n2. Download the SOC 2 Report In this lab we\u0026rsquo;ll look at the Service Organization Controls (SOC) 2 Report.\nAs you can see reports available include our Service Organization Control (SOC) reports, ISO, Payment Card Industry (PCI) reports, and certifications from accreditation bodies across geographies and compliance verticals that validate the implementation and operating effectiveness of AWS security controls. Type SOC 2 into the search and select SOC 2 Report - Current, then select Download report.\nRead and accept the NDA - Accept NDA and download\n3. Open the Report Open your downloads folder and open the file Service_Organization_Controls_(SOC)_2_Report_-_Current.pdf\nReview the Terms and Conditions of the artifact and click the paperclip icon to display the attachments.\nFor this report there are three files attached, as a start open the first file in the list, the AWS SOC 2 Type 2 Report.\nThe dates in the file name are the in scope dates for the report.\nSOC reports are audits performed over a period of time and do not expire. Our auditors perform our SOC audits twice a year over a period of 6 months: Oct 1 to Mar 31 and Apr 1 to Sept 30. Once the audit period is over, our auditors prepare their audit report which is then released in May and November, respectively. Should you seek assurance that we have maintained the control environment described in this most recent SOC report, we make a SOC Continued Operations Letter available to you in Artifact.\n 4. Review the Report Take some time to review these independent third-party examination reports that demonstrate how AWS achieves key compliance controls and objectives.\nMore information on our SOC compliance can be found at SOC Compliance.\n"},{"uri":"http://paulJRCurtis.github.io/risk-lab-site/4-detective-controls-config/1-config-setup/","title":"Enable AWS Config","tags":[],"description":"","content":"First we need to enable AWS Config and begin tracking of your resources.\n1. Login to your AWS Account If you haven\u0026rsquo;t already logon to your AWS account. If you don\u0026rsquo;t yet have an account go to Start the Workshop\n2. Access the AWS Config Console Select the Services dropdown on the menu bar at the top left. Select Config under Management \u0026amp; Governance.\nIf this is your first time using AWS Config\nSelect Get started from the AWS Config intro screen.\nIf you’ve already used AWS Config\nYou will start at the AWS Config Dashboard, select Settings on the menu to the left.\n3. Select resources to track On the Settings page, under Resource types to record, select Record all resources supported in this region checkbox and check the checkbox for Include global resources. Checking these two boxes means that AWS Config will record configuration changes for all supported resources all resources as well as configuration changes for AWS Identity and Access Management (IAM) resources which are global resources. Global resources are not tied to an individual region and can be used in all regions.\n4. Create a Bucket for configuration history AWS Config needs a place to store configuration history and configuration snapshot files, we will use Amazon Simple Storage Service (Amazon S3) to provide this storage space - what AWS calls an S3 Bucket.\nUnder Amazon S3 bucket, select Create a bucket to have the Amazon S3 bucket created automatically.\nThis S3 bucket will be used to store the history used by Config, but to keep things simple and remove the need to create a second Bucket, we will be applying the Config Rules to this same S3 Bucket.\n 5. Optional - Set-up notifications You can leave the settings under Amazon SNS topic as is, or as an extra activity later you can check the box for Stream configuration changes and notifications to an Amazon SNS topic, and then select Create a topic, and give the SNS Topic a name, something like AWS-Config-Alerts, or whatever you like.\nThis will enable AWS Config to send us configuration changes and notifications via Amazon Simple Notification Service (SNS). We will configure this later in an optional step.\n6. Check AWS Config access There is no need to change any settings in this section, Create AWS Config service-linked role should already be selected.\nIt is worth taking a moment to understand that for AWS Config to access other services like S3 and SNS we need to grant it permissions. This step crates a role for Config that will allow Config to access the S3 bucket and the SNS topic we set-up in the steps above.\n Click Next.\n"},{"uri":"http://paulJRCurtis.github.io/risk-lab-site/conformance-packs/1-introduction/","title":"Introduction","tags":[],"description":"","content":"In this course, we will be deploying Amazon S3 Operational Best Practices with remediation actions conformance pack in an AWS Account. This pack contains following AWS Managed Config Rules.\nYou can click on the AWS Managed Config Rule name to see the Developer Guide for the rule.\n  S3BucketPublicReadProhibited with Remediation Action\nChecks that your Amazon S3 buckets do not allow public read access. The rule checks the Block Public Access settings, the bucket policy, and the bucket access control list (ACL).\n  S3BucketPublicWriteProhibited with Remediation Action\nChecks that your Amazon S3 buckets do not allow public write access. The rule checks the Block Public Access settings, the bucket policy, and the bucket access control list (ACL).\n  S3BucketReplicationEnabled\nChecks whether S3 buckets have cross-region replication enabled.\n  S3BucketSSLRequestsOnly\nChecks whether S3 buckets have policies that require requests to use Secure Socket Layer (SSL).\n  S3BucketServerSideEncryptionEnabled with Remediation Action\nChecks that your Amazon S3 bucket either has Amazon S3 default encryption enabled or that the S3 bucket policy explicitly denies put-object requests without server side encryption.\n  S3BucketLoggingEnabled with Remediation Action\nChecks whether logging is enabled for your S3 buckets.\n  "},{"uri":"http://paulJRCurtis.github.io/risk-lab-site/pipeline-controls-intro/1-get-source-files/","title":"Get Source Files","tags":[],"description":"","content":" If you have already downloaded the risk kab source files you can skip this step.\n The very first thing we need to do is get the source files we will be using for this lab.\nThe files we\u0026rsquo;ll be using are available on github Risk-Labs. For this lab we\u0026rsquo;ll be using the files in the pipeline-controls-intro directory, There are five files but you can disregard the README.md file.\nThe four files we\u0026rsquo;ll be using are;\n cfn-network.yaml\nThis is the CloudFormation script to create two Amazon Virtual Private Clouds (VPCs), one for our test environment and one for our production environment. cfn-pipeline.json\nThis is the CloudFormation script that will build our AWS CodePipeline. codepipe-single-sg.zip\nThis file contains the CloudFormation scripts that is the change we are testing. It is a simple security group - A security group acts as a virtual firewall for your instance to control inbound and outbound traffic, and in this lab we are going to make sure it is not open to the world before we allow it into production. codepipeline-lambda.zip\nThis file contains the code the code pipeline will run to execute the test against our change.  If you have git installed you can simply clone the repository, otherwise you can download a zip file containing the files and unzip it into a local working directory.\nTo download a zip file with the code for this and the other labs in this series, click on the Code and choose Download ZIP.\n"},{"uri":"http://paulJRCurtis.github.io/risk-lab-site/3-aws-compliance-reports/2-interpret-results/","title":"Interpreting the results","tags":[],"description":"","content":"Now that we have the report lets take some time to understand it. First we need to understand what the report covers, the overall conclusion and the detailed results. We\u0026rsquo;ll start with the AWS Shared Responsibility Model and then dive into the specifics of the report.\n1. Shared Responsibility Model First we need to consider the report in the context of the Shared Responsibility Model i.e. what is an AWS responsibility and what is a customer responsibility. Controls can be grouped into three categories. Broadly speaking we can see that the model is divided into “Security of the Cloud” - AWS responsibility and “Security in the Cloud” – Customer responsibility. From a controls perspective this can be thought of as.\n Inherited Controls – Controls which a customer fully inherits from AWS for example physical and environmental controls. Shared Controls – Controls which apply to both the infrastructure layer and customer layers, but in completely separate contexts or perspectives. In a shared control, AWS provides the requirements for the infrastructure and the customer must provide their own control implementation within their use of AWS services. Examples include Patch Management – AWS is responsible for patching and fixing flaws within the infrastructure, but customers are responsible for patching their guest OS and applications. Customer Specific – Controls which are solely the responsibility of the customer based on the application they are deploying within AWS services. Examples include Service and Communications Protection or Zone Security which may require a customer to route or zone data within specific security environments.  Take a few minutes to familiarize yourself with the Shared Responsibility Model.\nTo support a deeper understanding of security and shared responsibility for AWS’ services, AWS has categorized them into three main categories: infrastructure, container, and abstracted. Each category comes with a slightly different security ownership model based on how customers interact and access the functionality.\nTo dive deeper into this topic you can complete to optional lab section Advanced Responsibility Model\n 2. Completeness Check Under the Scope heading of SECTION II – Independent Service Auditor’s Assurance Report there is a list of the AWS Services, and Global Infrastructure Locations which are considered in scope.\n3. Auditor Opinon Review the Opinion in SECTION II – Independent Service Auditor’s Assurance Report which provides the Auditor opinion of the report.\n4. The Detailed Results SECTION IV – Description of Criteria, AWS Controls, Tests and Results of Tests contains tables mapping between the Auditors Control Criteria and AWS Control Activities (AWSCA) and for each the results of the tests.\nCongratulations! you have accessed AWS Artifact, downloaded a SOC 2 Type 2 Report, and reviewed the results to confirm the controls that AWS operates on your behalf.\n "},{"uri":"http://paulJRCurtis.github.io/risk-lab-site/4-detective-controls-config/2-select-rules/","title":"Select AWS Config Rules","tags":[],"description":"","content":"1. Select Config Rules At the time this lab was being created there were 150 AWS Managed Config Rules to choose from but the number is growing.\nIn this introductory lab we will use three AWS Managed Config Rules that relate to S3. Recall that we created an S3 bucket already as part of the AWS Config setup to provide config a place to store configuration history and configuration snapshots.\nThe rules we will use are;\n s3-bucket-server-side-encryption-enabled s3-bucket-public-write-prohibited s3-bucket-public-read-prohibited  Enter s3-bucket into the search bar and select the three rules listed above and click Next\n2. Review Config Rules Click Confirm\n"},{"uri":"http://paulJRCurtis.github.io/risk-lab-site/conformance-packs/2-setup-prerequisite-resources/","title":"Setup Prerequisite Resources","tags":[],"description":"","content":"We will create prerequisite resources required for “Amazon S3 Operational Best Practices with Remediation Actions” conformance pack. This includes service linked role for Conformance Packs, Remediation action automation assume role and S3 Service Side logging bucket.\n1. Download required templates template\nThe files we\u0026rsquo;ll be using are available on github Risk-Labs. For this lab we\u0026rsquo;ll be using the files in the conformance-packs directory, There are three files but you can disregard the README.md file.\nThe two files we\u0026rsquo;ll be using are;\n prerequisite-resources.yaml\nThis is the CloudFormation script to create the resources that we will need to run the conformance pack. s3-best-practices-with-remediation-pack.yaml\nThis is the script that contains the AWS Config rules and the remediation scripts.  If you have git installed you can simply clone the repository, otherwise you can download a zip file containing the files by clicking Code and then unzip the file into a local working directory. Either way the risk labs code repository is on Github repo.\n2. Login to your AWS Account If you haven\u0026rsquo;t already logon to your AWS account\n3. Go CloudFormation\nTake the time (3:01) to watch the short video describing CloudFormation.\n  From the AWS Management Console, select Services in the top left and select CloudFormation under Management and Governance.\nClick Create stack.\nUnder Prerequisite - prepare template, ensure \u0026ldquo;Template is ready\u0026rdquo; is selected.\nUnder Specify template, select Upload a template file and click Choose file button.\nSelect the CloudFormation template prerequisite-resources.yaml downloaded in Step 1.\nClick Next\n4. Enter a Stack name\nSomething like \u0026ldquo;conformance-lab-prerequisites\u0026rdquo; makes sense but it can be whatever you like.\nClick Next\n5. Configure stack options\nLeave all defaults on the Configure stack options page and click Next\n6. Review stack In the Capabilities section at the bottom of the page, check the check box I acknowledge that AWS CloudFormation might create IAM resources with custom names.\nClick Create stack\n7. Stack creation complete\nOnce the deployment of the stack has completed you will see CREATE_COMPLETE on the left-hand side under Stacks. Take some time to explore the tabs, particularly the Events and Resources tabs. "},{"uri":"http://paulJRCurtis.github.io/risk-lab-site/pipeline-controls-intro/2-create-repo-bucket/","title":"Create Repo Bucket","tags":[],"description":"","content":"For this lab we are going to use an Amazon S3 bucket as our code repository. S3 provides all the functionality we need and doesn\u0026rsquo;t require us to manage \u0026ldquo;commits\u0026rdquo; that code repositories do, however you could use AWS CodeCommit or Git. Create your own S3 bucket in the desired region and enable versioning on the bucket.\n1. Log on to the AWS Console\n2. Select the Services dropdown in the top left or search for S3.\n3. Create a bucket\nSelect \u0026#43; Create bucket\nGive your bucket a name, something that will indicate that it\u0026rsquo;s your code repository is good but you can call it whatever you like.\nAmazon S3 has a global namespace. (i.e. No two S3 buckets can have the same name.) Therefore, you need to use a unique bucket name when creating S3 buckets. Before you start creating S3 buckets, it\u0026rsquo;s important to first understand valid syntax for bucket names as well as best practices. Bucket names should be lower-case, use \u0026lsquo;-\u0026rsquo; rather than spaces and should not include \u0026lsquo;.\u0026rsquo; or \u0026lsquo;_\u0026rsquo;.\n Click Next\nOn the Properties page turn on Versioning by selecting the checkbox and click Next again.\nThe next screen shows you the buckets public access settings, as you can see all public access is blocked by default. Leave these settings as they are and click Next\nFinally, click Create bucket\n4. Upload files to S3\nCopy the following files that you downloaded in the previous step to the new S3 Bucket;\n cfn-network.yaml cfn-pipeline.json codepipeline-lambda.zip  Click on the name of your new bucket and then click Upload. You can now drag and drop the files listed above to the bucket.\n5. Copy the Object URL\nCopy the Object URLs for the cfn-network.yaml and cfn-pipeline.json objects - we will need them in the following steps. To do this simply click on the file name and note the Object URL at the bottom of the screen.\n"},{"uri":"http://paulJRCurtis.github.io/risk-lab-site/3-aws-compliance-reports/3-optional-shared-responsibility/","title":"Optional: Shared Responsibility","tags":[],"description":"","content":"AWS groups services into three main categories: infrastructure, container, and abstracted. Each category comes with a slightly different security ownership model based on how customers interact and access the functionality. Customer responsibility is determined by the AWS Cloud services that a customer selects. This determines the amount of configuration work the customer must perform as part of their security responsibilities.\nInfrastructure Services: Services such as Amazon Elastic Compute Cloud (Amazon EC2) and Amazon Virtual Private Cloud (Amazon VPC) are categorized as Infrastructure Services and, as such, require the customer to perform the necessary security configuration and management tasks. If a customer deploys an Amazon EC2 instance, they are responsible for management of the guest operating system (including updates and security patches), any application software or utilities installed by the customer on the instances, and the configuration of the AWS-provided firewall (called a security group) on each instance.\nContainer Services: Container Services in this context is not to be mistaken with technologies like Docker or Kubernetes.\n Services in this category typically run separately on Amazon EC2 or other infrastructure instances, but sometimes customers are not required to manage the operating system or the platform layer. AWS provides a managed service for these application “containers”. Customers are responsible for setting up and managing network controls, such as firewall rules, and for managing platform-level identity and access management separately from IAM. Examples of container services include Amazon Relational Database Services (Amazon RDS), Amazon Elastic Map Reduce (Amazon EMR) and AWS Elastic Beanstalk.\nAbstracted Services: This category includes high-level storage, database, and messaging services, such as Amazon Simple Storage Service (Amazon S3), Amazon Glacier, Amazon DynamoDB, Amazon Simple Queuing Service (Amazon SQS), and Amazon Simple Email Service (Amazon SES). These services abstract the platform or management layer on which the customers can build and operate cloud applications. The customers access the endpoints of these abstracted services using AWS APIs, and AWS manages the underlying service components or the operating system on which they reside.\nTake advantage of AWS controls As every customer is deployed differently in AWS, customers can take advantage of shifting management of certain IT controls to AWS which results in a (new) distributed control environment. Customers can then use the AWS control and compliance documentation available to them to perform their control evaluation and verification procedures as required.\nMore information and examples refer to the AWS Security Best Practices Whitepaper.\nAWS also publishes security blogs related to best practices that covers best practices around using AWS services.\n"},{"uri":"http://paulJRCurtis.github.io/risk-lab-site/conformance-packs/3-enable-aws-config/","title":"Enable AWS Config","tags":[],"description":"","content":"We need to enable AWS Config and begin tracking of your resources.\n1. Access the AWS Config Console Select the Services dropdown on the menu bar at the top left. Select Config under Management \u0026amp; Governance.\nIf this is your first time using AWS Config\nselect Get started.\nIf you’ve already used AWS Config select Settings**\n2. Select Resources to track On the Settings page, under Resource types to record, select Record all resources supported in this region checkbox and check the checkbox for Include global resources. Checking these two boxes means that AWS Config will record configuration changes for all supported resources all resources as well as configuration changes for AWS Identity and Access Management (IAM) resources which are global resources. Global resources are not tied to an individual region and can be used in all regions.\n3. Create a Bucket for configuration History AWS Config needs a place to store configuration history and configuration snapshot files, we will use Amazon Simple Storage Service (Amazon S3) to provide this storage space - what AWS calls an S3 Bucket.\nUnder Amazon S3 bucket, select Create a bucket to have the Amazon S3 bucket created automatically.\n4. Set-up Notifications Optional - Under Amazon SNS topic, check the box for Stream configuration changes and notifications to an Amazon SNS topic, and then select Create a topic, and give the SNS Topic a name. Something like AWS-Config-Alerts, or what ever you like.\nThis will enable AWS Config to send us configuration changes and notifications via Amazon Simple Notification Service (SNS)\n5. Check AWS Config access There is no need to change any settings in this section, Create AWS Config service-linked role should already be selected.\nIt is worth taking a moment to understand that for AWS Config to access other services like S3 and SNS we need to grant it permissions. This step crates a role for Config that will allow Config to access the S3 bucket and the SNS topic we set-up in the steps above.\n Click Next.\nClick Next again.\nClick Skip to go to Review page and Click Confirm.\nYou can now progress to the next step.\n"},{"uri":"http://paulJRCurtis.github.io/risk-lab-site/4-detective-controls-config/3-dashboard/","title":"Review Dashboard","tags":[],"description":"","content":"1. Explore the Dashboard Take a few minutes to explore the information on the Dashboard.\nOn the right, in the Compliance status window you can see that there are a total of 3 rules - 1 Noncompliant and 2 Compliant, you can also see that there is 1 resource which is noncompliant.\nThe Noncompliant rules by noncompliant resource count lists noncompliant rules listed by the number of non-compliant resources. In this case there is only one noncompliant rule and a single resource.\nTo the left is the Resource Inventory, this lists the resources in the account. In this case the total shown is 32 resources. Note that there is only 1 S3 Bucket and our three rules all relate to S3 Buckets so only a singe resource is counted in our compliance status.\n2. Review noncompliant rule(s) In the Noncompliant rules by noncompliant resource count window select S3-bucket-server-side-encryption-enabled. This will take you to a screen displaying the rule details and the Resources in scope.\n3. Drill into noncompliant resource Click on the bucket name under Resources in scope to display the details of the noncompliant bucket. "},{"uri":"http://paulJRCurtis.github.io/risk-lab-site/pipeline-controls-intro/3-create-environments/","title":"Create Environments","tags":[],"description":"","content":"In this step we will create two Amazon Virtual Private Clouds (VPCs) to represent our Testing and Development environments. A VPC is the basic building block for a network on AWS.\nWe could create the VPCs on the AWS console but it is better practice to deploy our pipeline using AWS CloudFormation Page.\n1. Open CloudFormation in the AWS Console\nTake the time (3:01) to watch the short video describing CloudFormation if you haven\u0026rsquo;t already seen in.\n  Select Services on the menu bar at the top of the page and select CloudFormation under Management \u0026amp; Governance.\n2. Select network CloudFormation script\nFirst, make sure you\u0026rsquo;re in the right region.\nThen Create stack\nSelect Template is ready and Amazon S3 URL Specify the template by selecting Amazon S3 URL as the source and paste in the Object URL for the cfn-network.yaml file you copied in the previous step.\nClick Next\nCloudFormation will now ask you to enter a StackName and BusinessOwner. For the StackName enter something like pipeline-controls-lab-environments. CloudFormation also asks for a business owner, It does this because we have set BusinessOwner as a Tag in our CloudFormation script. Tagging resources is an important part of managing resources.\n  More on tagging...   Amazon Web Services allows customers to assign metadata to their AWS resources in the form of tags. Each tag is a simple label consisting of a customer-defined key and an optional value that can make it easier to manage, search for, and filter resources. Although there are no inherent types of tags, they enable customers to categorize resources by purpose, owner, environment, or other criteria.\nWithout the use of tags, it can become difficult to manage your resources effectively as your utilization of AWS services grows. However, it is not always evident how to determine what tags to use and for which types of resources. The goal of this whitepaper is to help you develop a tagging strategy that enables you to manage your AWS resources more effectively.\nAWS Whitepaper - Tagging Best Practice\n  Enter your name as the Business Owner and click Next\nThere is no need to change anything on the next page, scroll to the bottom and click Next.\nScroll to the bottom of the final page and click Create stack.\nWhen the stack has been deployed you will see CREATE_COMPLETE on the left-hand panel. Click the refresh button to see the events. "},{"uri":"http://paulJRCurtis.github.io/risk-lab-site/conformance-packs/4-deploy-conformance-pack/","title":"Deploy Conformance Pack","tags":[],"description":"","content":"We will deploy the custom conformance pack which is similar to the sample template \u0026ldquo;Operational Best Practices for S3\u0026rdquo; but it includes some remediation.\n In a text editor, open the s3-best-practices-with-remediation-pack.yaml that you downloaded in Step 2 - Setup Prerequisite Resources.\nYou will need to note your account ID can find by clicking on the dropdown beside your account name on the top menubar.  Replace “ACCOUNT_ID_PLACEHOLDER” on line 34, 71, 130 and 170 with your Account Id.\nEnter the Account ID as a number only - with no dashes.\n Don\u0026rsquo;t forget to save your changes.\n Go to the Config Console and select Conformance Packs from left navigation panel   Click on Deploy conformance pack.\n  Under template details, select Template is ready and under template location, select Upload a template file and click Choose file button and select s3-best-practices-with-remediation-pack.yaml template you updated in Step 1 above.\nClick Next\n  Enter a Conformance Pack Name, something like \u0026ldquo;S3-conformance-pack\u0026rdquo; makes sense but you can name it whatever you like.\n  Choose \u0026ldquo;Select S3 Bucket\u0026rdquo; and select the S3 Bucket named awsconfigconforms-delivery-bucket-{YOUR-AWS-ACCOUNT-ID}. This bucket was created as part of prerequisite cloudformation template in previous section.\n   Under parameter, click on Add Parameter. Enter the Key as S3TargetBucketNameForEnableLogging and Parameter Value as s3serversideloggingbucket-{YOUR-AWS-ACCOUNT-ID} (again do not include any dashes in your account ID) and Click Next\n  Click Deploy conformance pack\nDeploying the Conformance Pack may take a few minutes.   "},{"uri":"http://paulJRCurtis.github.io/risk-lab-site/pipeline-controls-intro/4-create-pipeline/","title":"Create a Pipeline","tags":[],"description":"","content":"Now lets create the code pipeline. This will allow us to ensure all change into our production environment progress through our test environment and are pass the required checks.\nOur code pipeline will be made up of four stages;\n Commit Static Code Analysis Test Deployment Production Deployment  Like the VPCs, we could create the pipeline on the AWS console but just like everything else on the AWS Platform a code pipeline is defined by code so we will adhere to good practive and and deploy our pipeline using AWS Cloud Formation.\n1. Select pipeline CloudFormation script\nGo to the Stacks view by clicking the hamburger icon ☰ in the top left and select Stacks. Then click Create Stack - choose the \u0026ldquo;With new resources (standard)\u0026rdquo; option.\nSelect Template is ready and Amazon S3 URL then paste in the Object URL for cfn-pipeline.json you copied in the previous step.\nClick Next\n2. Specify stack details\nYou need to provide three pieces of information;\n  A stack name, something like pipeline-controls-lab-stack makes sense but you can name it anything you like.\n  The name of the S3 bucket that contains the cfn-pipeline.json file.\nIf you can\u0026rsquo;t recall the bucket name you can open a second console screen by clicking Services on the menu bar at the top left of the screen and right clicking on S3. Choose Open link in new tab. then click off of the popup services menu to return to the CloudFormation screen.\n   Your email address, this is used to provide you with email notifications when the CodePipeline runs.   Click Next\nYou will be taken to the Configure stack options screen - there is no need to make any changes on this page.\nClick Next\n3. Review stack\nAs this stack provisions IAM roles you are required to acknowledge these changes by selecting the check box in the Capabilities box at the bottom of the page.\nClick Create stack\n"},{"uri":"http://paulJRCurtis.github.io/risk-lab-site/conformance-packs/5-view-compliance-and-remediation/","title":"View Compliance and Remediation","tags":[],"description":"","content":"We will check compliance status for each rule in conformance pack and associated resources\n Once conformance pack is deployed, Click on conformance pack name to drill down into details. You can view list of rules and their compliance status.   Click on a rule name to see rule details.   Expand Resources in Scope section to see resources in scope and their compliance status. If there are any existing non-compliant resources, you can manually remediate them or wait for auto-remediation to kick in.\n  To see auto-remediation in action on a new resource, create a new s3 bucket using S3 Console. AWS Config will discover the resource and mark it as non-compliant if it is not following S3 Best Practices.\n  Go back to Conformance Pack details and select a rule with remediation action.\n  Expand Resources in Scope section to see newly created resource with its compliance status. If the resource is non-compliant, auto-remediation action will apply to resource within few minutes.\n  Refresh the page to see updated resource compliance status.\n  You can also manually remediate a resource by selecting resource and clicking Remediate.\n  Take some time to explore the rest of the Config pages and dashboards outside of conformance packs.\n  "},{"uri":"http://paulJRCurtis.github.io/risk-lab-site/4-detective-controls-config/5-remediate/","title":"Remediate","tags":[],"description":"","content":"In this step we will remediate the noncompliant Managed Config Rule S3-bucket-server-side-encryption-enabled. For the purposes of this lab we will remediate the S3 Bucket manually from the AWS Console but in a real production environment you would want to have a remediation script and possibly take advantage of one of the options to automate it\u0026rsquo;s application.\n1. The S3 Console First navigate to the S3 Console. To do select Services on the menu bar at the top of the page and select S3. You can can find S3 by either using the search or selecting it from under Storage. You will be taken to the S3 Console. On the S3 Console you will see the S3 Bucket listed, click on the bucket name.\n2. Update the Bucket configuration You are now on the page for the S3 Bucket from here you can see tabs to configure various elements of the bucket.\nThe Overview tabs shows the folders and files contained in the bucket. Select the Properties tab.\nOne of the properties you can control from this screen is the default encryption used for the bucket. There are three options; none, AES-256 and AWS-KMS.\nSelect AES-256 encryption and click Save. You will be returned to the Properties tab where you can see the Default encryption is now set to AES-256.\nYou have now successfully remediated the S3-bucket-server-side-encryption-enabled rule for this bucket. The rule will be re-run against the S3 bucket automatically as the rule is triggered by configuration changes but this may take a few minutes.\n "},{"uri":"http://paulJRCurtis.github.io/risk-lab-site/conformance-packs/","title":"Corrective controls with Conformance Packs","tags":["AWS Config"],"description":"","content":"In this builder session, learn how to build a solution that will evaluate your AWS resources for configuration compliance using AWS Config Conformance Packs. You will also learn how to improve your security posture by remediating non-compliant resources using automatic remediation actions.\nRequirements: You will need AWS IAM Managed Policy “arn:aws:iam::aws:policy/AdministratorAccess” or equivalent.\n Contents  Introduction   Setup Prerequisite Resources   Enable AWS Config   Deploy Conformance Pack   View Compliance and Remediation   Clean-up   Learn More   "},{"uri":"http://paulJRCurtis.github.io/risk-lab-site/pipeline-controls-intro/5-check-cf/","title":"Check CloudFormation","tags":[],"description":"","content":"You can now see the two stacks you have created on the left-hand panel.\nBy default the CloudFormation console will display the Events tab listing all the events which occurred as part of the CloudFormation deployment. When the CloudFormation script has completed you will see CREATE_COMPLETE on the left hand panel.\nSwitch between the two stacks and explore the other tabs, in particular review the Resources tab to see all the AWS resources created by the CloudFormation template.\nGo to the Output tab for the pipeline-controls-lab-environments and take note of the VPC name for the Test and Production Environments. You will need these in Step 7.\n You should also now check your email, you will have received an email from AWS Notifications asking you to confirm your subscription to notifications from the pipeline. You can go ahead and confirm.\n"},{"uri":"http://paulJRCurtis.github.io/risk-lab-site/conformance-packs/6-clean-up/","title":"Clean-up","tags":[],"description":"","content":"It is important to turn off and delete unused resources so that they do not accrue charges.\n Go to Config console. Under settings page, Turn off recording. Delete any Conformance Packs you created. Empty the S3 bucket awsconformance-delivery-bucket-{ACCOUNT_ID} Go to CloudFormation console and delete the stack you created - this might take a few minutes. Go to S3 Console and delete any additional S3 buckets you created.  "},{"uri":"http://paulJRCurtis.github.io/risk-lab-site/4-detective-controls-config/6-timelines/","title":"Explore Timelines","tags":[],"description":"","content":"1. Return to Config Select Services on the menu bar at the top of the page and select Config. You can can find Config by either using the search or selecting it from under Management \u0026amp; Governance. 2. Access Resource timelines From the Resource screen click the Resource Timeline in the top right. 3. Review Resource configuration timeline You should now see the Configuration timeline tab of the timeline page for the resource.\nOn this tab we can see two entries on the timeline. The first @ 1:10:43 PM is the Resource discovery time and has two Events. Click on the Events hyperlink for this first entry on the timeline or scroll down the page.\nThis shows that the S3 bucket was created as was a PutBucketPolicy at 1:10:11 PM and provides a link to AWS CloudTrail. CloudTrail provides event history of your AWS account activity, including actions taken through the AWS Management Console, AWS SDKs, command line tools, and other AWS services.\nReturn now to the timeline a select the second entry on the timeline @ 5:17:37 PM and scroll down the page. You can see that this entry on the timeline logs the application of encryption we did during the remediation step.\n4. Review Resource configuration timeline Select the Compliance timeline tab and take a few minutes to explore the compliance timeline. This timeline logs the same events and changes but provides a compliance view of the resource. "},{"uri":"http://paulJRCurtis.github.io/risk-lab-site/pipeline-controls-intro/6-review-pipeline/","title":"Review Code Pipeline","tags":[],"description":"","content":"Go to Services and select CodePipeline under Developer Tools\nNote that under \u0026ldquo;Most recent execution\u0026rdquo; nothing is shown. This is because we haven\u0026rsquo;t yet provided a change for our code pipeline to process.\nClick on the name of your pipeline to see the stages.\nAgain, note that the first stage has Failed, but don\u0026rsquo;t worry it is only because the code pipeline tried to run when it is first provisioned but there was no file for it to process. We\u0026rsquo;ll provide a file soon.\nYou should have also received and email titled \u0026ldquo;AWS Notifications\u0026rdquo;, this was generated by the pipeline and will require you to confirm your subscription, which you should do now.\nCongratulations, you now have a functioning code pipeline! Now lets try it out.\n"},{"uri":"http://paulJRCurtis.github.io/risk-lab-site/conformance-packs/7-learn-more/","title":"Learn More","tags":[],"description":"","content":"Learn more about AWS Config conformance packs\n Conformance Pack Launch Announcement Conformance Pack Developer Guide Conformance Pack Sample Templates Managing Conformance Packs Across all Accounts in Your Organization  "},{"uri":"http://paulJRCurtis.github.io/risk-lab-site/pipeline-controls-intro/7-depoly-first-chenge/","title":"Deploy First Change","tags":[],"description":"","content":"1. Unzip the change file\nIn Step 1 - Get Source Files we downloaded the source files. Go to the directory which contains these files and unzip the codepipe-single-sg.zip file, it contains three files;\n single_security_group_one_cidr_ingress.json\nThis file is the main CloudFormation script for our change containing the configuration for the security group that we will deploy first into the Testing Environment, then into the Production Environment. test-stack-configuration.json\nThis file includes the name of the VPC used for our testing environment. prod-stack-configuration.json This file includes the name of the VPC used for the production environment.  2. Review the main CloudFormation template\nFirst, lets have a look at the single_security_group_one_cidr_ingress.json file. This file creates a security group - a security group acts as a virtual firewall for your instance to control inbound and outbound traffic.\nWe\u0026rsquo;re not going to get into the format or structure of the file in this lab but there are a few lines that we will focus on. Where the file provides the details for the SecurityGroupIngress you can see that the CidrIp range is set to 0.0.0.0/0 and the FromPort is 22. This means that the security group is open to remote connection from any ip address - which we do not want - but lets leave it as is for now.\n{ \u0026#34;AWSTemplateFormatVersion\u0026#34;: \u0026#34;2010-09-09\u0026#34;, \u0026#34;Description\u0026#34;: \u0026#34;Sample, test only template that creates a sample Security Group allowing all SSH traffic inbound from 0.0.0.0/0 . This template is for testing purposes only.\u0026#34;, \u0026#34;Parameters\u0026#34;: { \u0026#34;VPCName\u0026#34;: { \u0026#34;Description\u0026#34;: \u0026#34;ID of the existing VPC.\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;AWS::EC2::VPC::Id\u0026#34; } }, \u0026#34;Resources\u0026#34;: { \u0026#34;sg\u0026#34;: { \u0026#34;Type\u0026#34; : \u0026#34;AWS::EC2::SecurityGroup\u0026#34;, \u0026#34;Properties\u0026#34; : { \u0026#34;GroupDescription\u0026#34; : \u0026#34;SSH Security Group\u0026#34;, \u0026#34;SecurityGroupIngress\u0026#34; : { \u0026#34;CidrIp\u0026#34;: \u0026#34;0.0.0.0/0\u0026#34;, \u0026#34;FromPort\u0026#34;: 22, \u0026#34;ToPort\u0026#34;: 22, \u0026#34;IpProtocol\u0026#34;: \u0026#34;tcp\u0026#34; }, \u0026#34;Tags\u0026#34;: [ {\u0026#34;Key\u0026#34;: \u0026#34;Name\u0026#34;, \u0026#34;Value\u0026#34;: \u0026#34;Immersion-Day-Demo\u0026#34;}, {\u0026#34;Key\u0026#34;: \u0026#34;LOB\u0026#34;, \u0026#34;Value\u0026#34;: \u0026#34;Risk-\u0026amp;-Compliance\u0026#34;} ], \u0026#34;VpcId\u0026#34; : { \u0026#34;Ref\u0026#34; : \u0026#34;VPCName\u0026#34; }} } } } 3. Update the CloudFormation templates\nNext we will update the two configuration files with the names of the VPCs we noted in Step 5 - Check CloudFormation.\n test-stack-configuration.json\nUpdate the VPCName to the name of your test environment VPC. prod-stack-configuration.json Update the VPCName to the name of your production environment VPC.  Your test-stack-configuration.json and prod-stack-configuration.json files should look like this but with your vpc as the value for VPCName.\n{ \u0026#34;Parameters\u0026#34; : { \u0026#34;VPCName\u0026#34; : \u0026#34;vpc-cd0dd4a9\u0026#34; } } Don\u0026rsquo;t forget to save your changes.\n4. Create new codepipe-single-sg.zip file\nZip up the three files into a new codepipe-single-sg.zip\n5. Upload the CloudFormation script to the code repository bucket\nUpload the new codepipe-single-sg.zip file into the S3 bucket you created in Step 2 - Create Repo Bucket.\n6. Go to the Pipeline\nThe AWS CodePipeline will detect that the file is in the S3 bucket and will run automatically, or you can click on the pipeline name and click Release change\nYou can see that the StaticCodeAnalysis stage has failed, to see why click the Details link just below Failed.\nYou can see the execution failed due to \u0026ldquo;Failed filters \u0026lsquo;IngressOpenToWorld\u0026rsquo;, \u0026lsquo;SSHOpenToWorld\u0026rsquo;\u0026quot;. This is exactly the issue we saw when we reviewed the code earlier. To dive even deeper click Link to execution details. this will take you to Amazon CloudWatch.\n"},{"uri":"http://paulJRCurtis.github.io/risk-lab-site/pipeline-controls-intro/8-edit-change/","title":"Edit Change &amp; Redeploy","tags":[],"description":"","content":"In this step we will secure the CloudFormation template for our Security Group and redeploy it.\n1. Edit the Security group\nGo back to your working directory and edit the single_security_group_one_cidr_ingress.json file to a more restrictive ip range - 72.21.196.67/32\n{ \u0026#34;AWSTemplateFormatVersion\u0026#34;: \u0026#34;2010-09-09\u0026#34;, \u0026#34;Description\u0026#34;: \u0026#34;Sample, test only template that creates a sample Security Group allowing all SSH traffic inbound from 0.0.0.0/0 . This template is for testing purposes only.\u0026#34;, \u0026#34;Parameters\u0026#34;: { \u0026#34;VPCName\u0026#34;: { \u0026#34;Description\u0026#34;: \u0026#34;ID of the existing VPC.\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;AWS::EC2::VPC::Id\u0026#34; } }, \u0026#34;Resources\u0026#34;: { \u0026#34;sg\u0026#34;: { \u0026#34;Type\u0026#34; : \u0026#34;AWS::EC2::SecurityGroup\u0026#34;, \u0026#34;Properties\u0026#34; : { \u0026#34;GroupDescription\u0026#34; : \u0026#34;SSH Security Group\u0026#34;, \u0026#34;SecurityGroupIngress\u0026#34; : { \u0026#34;CidrIp\u0026#34;: \u0026#34;72.21.196.67/32\u0026#34;, \u0026#34;FromPort\u0026#34;: 22, \u0026#34;ToPort\u0026#34;: 22, \u0026#34;IpProtocol\u0026#34;: \u0026#34;tcp\u0026#34; }, \u0026#34;Tags\u0026#34;: [ {\u0026#34;Key\u0026#34;: \u0026#34;Name\u0026#34;, \u0026#34;Value\u0026#34;: \u0026#34;Immersion-Day-Demo\u0026#34;}, {\u0026#34;Key\u0026#34;: \u0026#34;LOB\u0026#34;, \u0026#34;Value\u0026#34;: \u0026#34;Risk-\u0026amp;-Compliance\u0026#34;} ], \u0026#34;VpcId\u0026#34; : { \u0026#34;Ref\u0026#34; : \u0026#34;VPCName\u0026#34; }} } } } Don\u0026rsquo;t forget to change your changes.\n2. Create new codepipe-single-sg.zip file\nZip up the three files into a new codepipe-single-sg.zip\n3. Upload the CloudFormation script to the code repository\nUpload the new codepipe-single-sg.zip file into the S3 bucket you created in Step 2 - Create Repo Bucket. There is no need to delete the old version of codepipe-single-sg.zip from the S3 Bucket first, S3 will replace the old file.\n4. Go to the Pipeline\nThe Pipeline will detect that the new file is in the S3 bucket and will run automatically, or you can click Release change\n5. Approve Test Stack\nThe change should now pass the validation steps and will progress to the ApproveTestStack stage.\nClick Review on the ApproveTestStack step in the pipeline.\nEnter a review comment and click Approve.\nWait for the final steps of the pipeline complete which should only take a few minutes.\n"},{"uri":"http://paulJRCurtis.github.io/risk-lab-site/pipeline-controls-intro/9-review-sg/","title":"Review Security Group","tags":[],"description":"","content":"On the console go to VPC which you\u0026rsquo;ll find in the Networking \u0026amp; Content Delivery section.\nClick on VPCs\nAbout halfway down the list on the left-hand side of the page you will see Security Groups - select it.\nSelect the checkbox beside Risk Lab Sec Group. Click on the Inbound rules to see that the rule is as we specified.\n"},{"uri":"http://paulJRCurtis.github.io/risk-lab-site/pipeline-controls-intro/10-clean-up/","title":"Clean Up","tags":[],"description":"","content":"1. Delete S3 artifactstorebucket\nWhen we ran the CloudFormation template to create the CodePipeline, one of the resources created was an S3 bucket the CodePipeline uses to store it\u0026rsquo;s working files. We need to empty this bucket in order for the CloudFormation deletion to complete.\nSelect the checkbox beside the bucket that\u0026rsquo;s name starts with pipeline-controls-lab-stack-artifactstorebucket- and click Empty\nYou can only delete empty S3 buckets. Deletion fails for buckets that have contents.\n 2. Delete CloudFormation Stacks\nGo to Cloudformation and delete;\n Prod-SG pipeline-controls-lab-environments pipeline-controls-lab-stack  "},{"uri":"http://paulJRCurtis.github.io/risk-lab-site/pipeline-controls-intro/","title":"Preventative controls with Code Pipeline","tags":[],"description":"","content":"A powerful way to implement preventative controls is to embed them into code pipelines.\nIn this lab we demonstrate a code pipeline which takes an AWS CloudFormation template and runs checks against it, if it passes the template is then deployed into a testing environment where additional checks are run, and finally if the template meets requirements it can be deployed into the production environment.\nContents  Get Source Files   Create Repo Bucket   Create Environments   Create a Pipeline   Check CloudFormation   Review Code Pipeline   Deploy First Change   Edit Change \u0026amp; Redeploy   Review Security Group   Clean Up   "},{"uri":"http://paulJRCurtis.github.io/risk-lab-site/tags/aws-config/","title":"AWS Config","tags":[],"description":"","content":""},{"uri":"http://paulJRCurtis.github.io/risk-lab-site/categories/beginner/","title":"Beginner","tags":[],"description":"","content":""},{"uri":"http://paulJRCurtis.github.io/risk-lab-site/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"http://paulJRCurtis.github.io/risk-lab-site/tags/detective-controls/","title":"Detective Controls","tags":[],"description":"","content":""},{"uri":"http://paulJRCurtis.github.io/risk-lab-site/tags/security-of-the-cloud/","title":"Security of the Cloud","tags":[],"description":"","content":""},{"uri":"http://paulJRCurtis.github.io/risk-lab-site/tags/","title":"Tags","tags":[],"description":"","content":""},{"uri":"http://paulJRCurtis.github.io/risk-lab-site/categories/intermediate/","title":"Intermediate","tags":[],"description":"","content":""},{"uri":"http://paulJRCurtis.github.io/risk-lab-site/categories/advanced/","title":"Advanced","tags":[],"description":"","content":""}]