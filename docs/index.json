[{"uri":"/1-start-workshop.html","title":"Start the Workshop","tags":[],"description":"","content":"To start the workshop, follow one of the following depending on whether you are;\nrunning the workshop on your own (in your own account),\nor\nattending an AWS hosted event (using AWS provided hashes)\n"},{"uri":"/7-preventative-controls-codepipeline/1-intro.html","title":"Introduction","tags":[],"description":"","content":"An important aspect of infrastructure as code is testing and the ability to ensure it meets standards and control requirements before it reaches production.\nIn this workshop, we will pass a CloudFormation script written in a language called yaml through AWS CodePipeline. If the CloudFormation script fails any of the checks we will investigate why and correct these. Once the script passes all checks it will be deployed into our environment.\nThe Pipeline has three stages;\n Source - monitor the S3 Bucket for any changes to the file cfn-template.yaml.zip and when detected pass the changes to the Build stage. Note that in a production environment you would use a code repository tool rather than S3, but in this case we will keep it simple. Build - take the cfn-template.yaml.zip file from the source stage, run it through the CloudFormation checks, produce a report, and if the CloudFormation template passes all the checks, then moves the file on to the Deploy stage. Deploy - accept the script from the Build stage and passes it to AWS CloudFormation to deploy into our environment.    It\u0026rsquo;s helpful to understand that CodePipeline can have many different pipelines, each handling different kinds of jobs. These jobs can be the deployment of infrastructure like in this workshop, or applications.\nTo help demonstrate the pipeline, a CloudFormation Template to deploy two new S3 Buckets has been submitted, but the deployment has failed. The Engineering Team have invited you to review the pipeline and have a look at why the deployment has failed.\nIn this workshop we will demonstrate this idea by checking a CloudFormation script. The script we will test contains the instructions to create three S3 buckets.\nAs member of the Risk and Assurance Team at AnyCompany you have been engaged in an increasing number of discussions with the AnyCompany Cloud Engineering Team about their adoption of DevOps. To keep up you\u0026rsquo;ve been doing a bit of research including reading up on the AWS What is DevOps? page.\nYou know that core to the DevOps model is development and operations teams no longer operating in silos and at AnyCompany the two teams are now merged into a single team where engineers work across the entire application lifecycle, from development and test to deployment to operations. AnyCompany has adopted the mantra \u0026ldquo;You build it, you run it\u0026rdquo;. As a result you are seeing decreased defects, improvements in quality and faster development cycles.\nAs AnyCompany moves more workloads onto AWS, the Cloud Team is applying the same DevOps Model to infrastructure. You know that because infrastructure is described in CloudFormation templates it can be tested just like application code and changes made very quickly.\nYou are however concerned that security seems to be a bit of a bolt on at the end, and there\u0026rsquo;s a bit of increased tension between the Security Team and Engineering. With the Engineering Team complaining that they are being slowed down and that evidencing controls for risk sign-off is taking up too much time. You are also noticing some rising stress levels in the Security Team with the increased workload.\nYou need to find a way to automate the testing of as many security controls as you can. You also want to put much of this in the hands of the Engineering Team. This will allow the engineers to test against these controls early in the development cycle and as often as they like. Doing this will allow the Security Team and Risk and Assurance Team to focus on other testing other higher order controls. It will also make everyone\u0026rsquo;s life a lot easier.\nLearning Outcome This workshop provides an introduction to the implementation of preventative controls via a code pipeline. This is central to the DevOps idea of \u0026lsquo;shifting left\u0026rsquo;, or moving testing earlier in the development cycle and having it performed more frequently. In this challenge you will learn how to \u0026lsquo;shift left\u0026rsquo; a set of infrastructure as code tests to ensure that the CloudFormation scripts meet requirements prior to being deployed.\nThis workshop will also introduce YAML, one of the AWS CloudFormation Template formats.\nTechnical knowledge prerequisites This workshop doesn\u0026rsquo;t require familiarity with AWS services, however being able to navigate the console and some basic understanding of S3 will be helpful. You will need to be able to download files, zip and unzip files, and open files in a text editor.\nTopics covered By completing this workshop, you will be able to:\n Review an AWS CodePipeline and understand it\u0026rsquo;s progress, Navigate AWS CodeBuild and review it\u0026rsquo;s logs and reports, and Update a CloudFormation template.  "},{"uri":"/2-know-the-console.html","title":"Getting to Know the Console","tags":[],"description":"","content":"Login to your AWS Account If you haven\u0026rsquo;t already, logon to your AWS account. If you don\u0026rsquo;t have an account, go to Start Workshop.\nLet\u0026rsquo;s get to know the AWS Management Console\n  The AWS Management Console is a web application that comprises and refers to a broad collection of service consoles for managing Amazon Web Services. When you first sign in, you see the console home page.\nAlong the top menubar you will see a few important elements we will use often.\nAWS Logo Clicking on the AWS logo will bring you back to this console screen.\nServices Dropdown Choose Services to open a full list of services. On the upper right of the page, choose Group to see the services listed by category or choose A–Z to see an alphabetical listing. Then choose the service that you want. Clicking Services again will close the list.\nAlert Clicking on alerts will display a dropdown showing Open issues, scheduled changes and Other notifications, as well as the option to show all alerts.\nClicking any of these will take you to the Personal Health Dashboard. Personal Health Dashboard gives you a personalized view into the performance and availability of the AWS services underlying your AWS resources, to learn more go to AWS Personal Health Dashboard.\nUser/Account Dropdown The User/Account Dropdown is on which we will use on occasionally during the labs. Lets step through the information on the dropdown. User - the image below shows the Federated Login that you will see if your identity is provided to the AWS Console form an external identity provider - in the example show this is the AWS Event Engine. If you signed on directly to the console you will see IAM User Account: this is the account Id, the example shows a number but you can also setup and account alias which is easier to remember. The next grouping of options may vary slightly depending on your access but they deal with billing and your credentials. The final option is to Sign Out.\nRegion Dropdown For many services, you can choose a Region that specifies where your resources are managed. You do not choose a Region for the AWS Management Console or for some services, such as IAM. To choose a Region\n In the AWS Management Console, choose a service to go to that service\u0026rsquo;s console. On the navigation bar, choose the name of the currently displayed Region. When you choose a Region, that Region becomes the default in the console.  If you have created AWS resources, but you don’t see those resources in the console, the console might be displaying resources from a different Region. Some resources (such as EC2 instances) are created in a specific Region. To see them, use the Region selector to choose the Region in which the resources were created.\n Support Dropdown This dropdown lists the various support options and resources available to you.\n"},{"uri":"/1-start-workshop/on-your-own/create-an-aws-account.html","title":"Create an AWS Account","tags":[],"description":"","content":" Your account must have the ability to create new IAM roles and scope other IAM permissions.\n   If you don\u0026rsquo;t already have an AWS account with Administrator access: create one now by clicking here\n  Once you have an AWS account, ensure you are following the remaining workshop steps as an IAM user with administrator access to the AWS account: Create a new IAM user to use for the workshop\n  Enter the user details:\n    Attach the AdministratorAccess IAM Policy:    Click to create the new user:    Take note of the login URL and save:    "},{"uri":"/1-start-workshop/at-an-aws-event/aws-workshop-portal.html","title":"AWS Workshop Portal","tags":[],"description":"","content":"Login to AWS Workshop Portal This workshop creates an AWS account. You will need the Participant Hash provided upon entry, and your email address to track your unique session.\nUse Chrome or Firefox to ensure a good experience.\n 1. Go to Event Engine Connect to the portal by clicking the button or browsing to https://dashboard.eventengine.run. The following screen will be displayed.\n  2. Enter Hash Code Enter the provided hash in the text box. The button on the bottom right corner changes to Accept Terms \u0026amp; Login. Click on that button to continue.\n  3. Open AWS Console Click on AWS Console on dashboard.\n  Click on Open AWS Console. This will open AWS Console in a new browser tab. You will not need the Credentials / CLI Snippets for these workshops.\nThe image above shows a reminder to only use the \u0026ldquo;ap-southeast-2\u0026rdquo; region, your workshops will deploy in the region you should use.\n "},{"uri":"/1-start-workshop/on-your-own.html","title":"In your own account","tags":[],"description":"","content":" Only complete this section if you are running the workshop on your own. If you are at an AWS hosted event (such as an Immersion Day, etc), go to At an AWS Event\n Contents  Create an AWS Account   "},{"uri":"/3-aws-compliance-reports.html","title":"Accessing AWS Compliance Reports","tags":["Security of the Cloud","Artifact"],"description":"","content":"In this workshop you will access AWS Artifact and download a compliance report and work through guidance on how to interpret the report. This workshop is a great place to start if you are new to AWS, it mixes a bit of hands-on with supporting theory.\nDuration The lab should take approximately 30 minutes to complete.\nLearning Outcome This workshop provides an introduction to compliance at AWS including the Shared Responsibility Model. When you have completed this workshop you will be: familiar with the Shared Responsibility Model, and be able to access AWS’ security and compliance reports.\nTopics covered\n Shared Responsibility Model, AWS Compliance, and AWS Artifact  Target Audience This workshop has been developed specifically with the risk, compliance, and controls assurance community in mind, but anyone interested in understanding how to evidence AWS controls will benefit.\nPrerequisites Aside from and AWS Account, there are no prerequisites and no assumed AWS knowledge for this workshop. This is a great place to start for those new to AWS.\nCosts There are no costs associated with this lab.\nSteps  Introduction   Access a report   Interpreting the results   Deep Dive: Shared Responsibility   "},{"uri":"/4-detective-controls-config.html","title":"Detective Controls with Config","tags":["AWS Config","Detective Controls"],"description":"","content":"In this workshop we will use three AWS Config and AWS Config Rules to demonstrate how to automate controls, in this case, the controls will check the configuration of an Amazon Simple Storage Service (Amazon S3) bucket. The same approach can be applied to many AWS services.\nDuration The lab should take approximately 45 minutes to complete.\nOutcome By the completion of this lab you will have an understanding of the AWS Config Service and and understanding of the dashboard and timelines. You will also get an introduction to controls automation.\nTarget Audience This workshop has been developed specifically with the risk, compliance, and controls assurance community in mind, but anyone interested in understanding how to evidence AWS controls will benefit.\nPrerequisites Aside from and AWS Account, there are no prerequisites and no assumed AWS knowledge for this workshop.\nCosts There is a cost associated with AWS Config so it is important to ensure you clean up upon completion of this lab. For more information on the cost of AWS Config see the AWS Config Pricing page.\n The cost of this activity is expected to be \u0026lt; 0.50 USD.  Steps  Introduction   Set-up   Enable AWS Config   Add Managed Config Rules   Review Dashboard   Rule Scope   Auto Remediate   Explore Timelines   Clean up   "},{"uri":"/6-patching-controls-ssm.html","title":"Patching Controls with Systems Manager","tags":["AWS Systems Manager","Detective Controls"],"description":"","content":"In this workshop you will be introduced to immutability of instances, and use AWS Systems Manager to inventory the operating systems and software running in the environment, identify patching requirements, and set up a corrective controls which will patch the selected instances.\nDuration The lab should take approximately 60 minutes to complete.\nOutcome Learn to apply automated controls to patching and about the different approached for Immutable and non-Immutable infrastructure.\nIn this workshop you will learn how to move to automated patching controls, and how to use automation to gather information about the environment rather than relying on sample testing. You will get an introduction to the concept of immutable infrastructure, and an understanding of how traditional patching approaches should be replaced by maintaining an AMI, rather than maintaining many separate instances.\nTarget Audience This workshop has been developed specifically with the risk, compliance, and controls assurance community in mind, but anyone interested in understanding how to evidence AWS controls will benefit.\nPrerequisites Aside from and AWS Account, there are no prerequisites and no assumed AWS knowledge for this workshop.\nCosts There are no costs associated with this lab.\nSteps  Introduction   Set-up   Determine the current OS versions   Set patch baselines   Review compliance   Create a maintenance window   Create Patch Manager configuration   Patch Now   Clean up   "},{"uri":"/1-start-workshop/at-an-aws-event.html","title":"At an AWS Event","tags":[],"description":"","content":" Only complete this section if you are at an AWS hosted event (such as an Immersion Day, or any other event hosted by an AWS employee). If you are running the workshop on your own, go to: On your own.\n Contents  AWS Workshop Portal   "},{"uri":"/","title":"Risk &amp; Compliance Workshop","tags":[],"description":"","content":"Introduction These workshops have been developed specifically with risk, compliance, and controls assurance stakeholders in mind. AWS provides a wealth of services and tools to assist in effective management and governance and provides an unprecedented level of transparency. These workshops aim to demonstrate how these services and the telemetry available on the platform can be used to automate controls assurance and provide a real time risk data. These workshops have been designed to cater for the absolute beginner, building up to more advanced topics.\nThis repository contains documentation and code in the format of hands-on workshops to help you learn, measure, and build. Some workshops, particularly the introductory workshops maybe be simplified to demonstrate concepts rather than attempting to be production ready approaches. Where this is the case it will be noted in the lab.\nPrerequisites An AWS account that you are able to use for learning and experimenting, that is not used for production or other purposes. If you are doing these workshops at an AWS event you may be given an account.\nBeginner workshops For Beginner workshops there are no prerequisites and no assumed AWS knowledge, however the workshops to build on each other and should ideally be taken in sequence.\nIntermediate workshops For Intermediate and Advanced workshops you may require the following.\n A working directory where you have the rights to create files and directories. A text editor, we will be editing a few configuration files and scripts but not doing any major coding so a basic editor will do - notepad or TextEdit are fine. We will be using zip files so you will need to be able to un-zip and zip files.  "},{"uri":"/3-aws-compliance-reports/1-intro.html","title":"Introduction","tags":[],"description":"","content":"Compliance is a Shared Responsibility Security and Compliance is a shared responsibility between AWS and the customer. This shared model can help relieve the customer’s operational burden as AWS operates, manages and controls the components from the host operating system and virtualization layer down to the physical security of the facilities in which the service operates. The customer assumes responsibility and management of the guest operating system (including updates and security patches), other associated application software as well as the configuration of the AWS provided security group firewall.\n  See the Shared Responsibility Model for more.\nThis workshop focuses on introducing AWS Artifact and how customers can perform due-diligence using AWS’ auditor issued reports. AWS Artifact is your go-to, central resource for compliance-related information that matters to you. It provides on-demand access to AWS’ security and compliance reports and select online agreements.\nTo support a deeper understanding of security and shared responsibility for AWS’ services, AWS has categorized them into three main categories: infrastructure, container, and abstracted. Each category comes with a slightly different security ownership model based on how customers interact and access the functionality.\nTo dive deeper into this topic have a look at Deep Dive: Responsibility Model\n "},{"uri":"/4-detective-controls-config/1-intro.html","title":"Introduction","tags":[],"description":"","content":"AWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. Config continuously monitors and records your AWS resource configurations and allows you to automate the evaluation of recorded configurations against desired configurations.\nAt the time this workshop was being created there were 150 AWS Managed Config Rules to choose from, and the number is growing.\nWith Config, you can review changes in configurations and relationships between AWS resources, dive into detailed resource configuration histories, and determine your overall compliance against the configurations specified in your internal guidelines. This enables you to simplify compliance auditing, security analysis, change management, and operational troubleshooting.\nTake a minute (1:34) to watch the video below.\n  "},{"uri":"/6-patching-controls-ssm/1-intro.html","title":"Introduction","tags":[],"description":"","content":"Many organizations struggle to keep up with patching requirements. Sometimes one of the challenges is to get a definitive inventory of the operating systems and software running in the environment. In this workshop we will explore using AWS Systems Manager to gain visibility of the environment and automate patching.\nIn a cloud environment there are two different approaches to patching based on whether the architecture includes instances that are immutable or non-immutable. This might seem a confusing statement, but lets break it down. First, instances are the equivalent of servers in cloud speak. An immutable instance is one that is never changed or updated, it just gets replaced. When an patch exists for an immutable instance the image, or Amazon Machine Image (AMI), used to create the instance is update. The instances can be then be replaced with new instance created from the new AMI.\nImmutable infrastructure has some big benefits including greater infrastructure consistency, a more predictable deployment process, and the ability to easily scaled up and down to meet capacity requirements. This immutable approach cannot be applied to every instance as some legacy applications don\u0026rsquo;t allow for it. Where instances can not adopt this immutable approach they can be managed using AWS AWS Systems Manager - Patch Manager.\nIn this lab you will use Patch Manager to ensure instances meet patch baselines requirements.\nFor more information see our resources on Automated patching for non-immutable instances in the hybrid cloud using AWS Systems Manager\nIn this workshop you will get a clear view of the operating systems and software running in the environment, identify patching requirements, and set up a corrective controls which will patch the selected instances.\n"},{"uri":"/3-aws-compliance-reports/2-access-a-report.html","title":"Access a report","tags":[],"description":"","content":"1. Access the AWS Artifact Console From the Console select the Services dropdown and Artifact under Security, Identity, \u0026amp; Compliance.\n  Click View reports \n2. Download the SOC 2 Report In this workshop we\u0026rsquo;ll look at the Service Organization Controls (SOC) 2 Report.\nAs you can see reports available include our Service Organization Control (SOC) reports, ISO, Payment Card Industry (PCI) reports, and certifications from accreditation bodies across geographies and compliance verticals that validate the implementation and operating effectiveness of AWS security controls.\n  Enter \u0026ldquo;SOC 2\u0026rdquo; into the search and select SOC 2 Report - Current, then select Download report .\nRead and accept the NDA - Accept NDA and download \n3. Open the Report Open your downloads folder and open the file Service_Organization_Controls_(SOC)_2_Report_-_Current.pdf\nReview the Terms and Conditions of the artifact and click the paperclip icon to display the attachments.\n  For this report there are three files attached, as a start open the first file in the list, the AWS SOC 2 Type 2 Report.\nThe dates in the file name are the in scope dates for the report.\nSOC reports are audits performed over a period of time and do not expire. Our auditors perform our SOC audits twice a year over a period of 6 months: Oct 1 to Mar 31 and Apr 1 to Sept 30. Once the audit period is over, our auditors prepare their audit report which is then released in May and November, respectively. Should you seek assurance that we have maintained the control environment described in this most recent SOC report, we make a SOC Continued Operations Letter available to you in Artifact.\n 4. Review the Report Take some time to review these independent third-party examination reports that demonstrate how AWS achieves key compliance controls and objectives.\nMore information on our SOC compliance can be found at SOC Compliance.\n"},{"uri":"/4-detective-controls-config/2-env-setup.html","title":"Set-up","tags":[],"description":"","content":"In this Lab you will start with five AWS resources.\n Three example Amazon Simple Storage Service (Amazon S3) buckets or S3 Bucket for short. These S3 buckets are the example resources we will monitor with AWS Config. An IAM Role used by the Config auto-remediation to access the S3 Buckets. An IAM policy, which defines the access of the Iam Role.    1. Get to know CloudFormation Regardless of if you are using your own account an one supplied by AWS, the resources above are created using AWS CloudFormation. CloudFormation is an important service as it provides a way to rapidly and consistently deploy AWS resources. This is at the heart of the concept of infrastructure as code. Take the time (3:01) to watch the short video describing CloudFormation if you haven\u0026rsquo;t already seen in.\n  The following steps are required only if you are completing this workshop in your own AWS account. If you are using an AWS account which has been provided as part of a AWS Event (Event Engine) then the environment will be set up for you.\n 2. Create a CloudFormation stack Select Services on the menu bar at the top of the AWS Console and select CloudFormation under Management \u0026amp; Governance.\n  Click Create stack \n  Under Prerequisite - Prepare template ensure Template is ready is selected, this will tell CloudFormation you have a pre-prepared CloudFormation script.\nUnder Specify template select Upload a template file and choose the detective-controls-config.yaml file.\nTODO: Provide location of - detective-controls-config.yaml\n 3. Specify stack details CloudFormation will now ask you to enter a StackName, enter something like config-workshop.\n  Click Next \nThere is no need to change anything on the Configure stack options page, scroll to the bottom and click Next .\nScroll to the bottom of the final page, under Capabilities you will see a message box telling you that the CloudFormation template will create Identity and Access Management (IAM) resources, this is just a warning to ensure you don\u0026rsquo;t inadvertently compromise your security by creating unwanted permissions.\n  Select the acknowledgement check box and click Create stack . 4. Check progress You will be taken bck to the CloudFormation screen for the stack you are creating. Here you can follow the progress of the creation of resources.\n  When the stack has been deployed you will see CREATE_COMPLETE on the left-hand panel. Click the refresh button to see the events.\nClick on the Resources tab to see the list of resources created.\n  You have successfully created the resources you need to complete this workshop. you should progress to the next step.\nMore about these later. Click Next \n"},{"uri":"/6-patching-controls-ssm/2-env-setup.html","title":"Set-up","tags":[],"description":"","content":"In this Lab you will start with the following AWS resources.\n an Amazon Virtual Private Cloud (VPC) with a single public subnet, a security group, and two EC2 fleets; the first with three EC2 instances running the Amazon Linux 2 operating system and the second with two instances running Windows.  An EC2 Fleet is a way of grouping and managing EC2 instances and provisioning using the lowest price combination of instances available. You can learn more about Fleets on the Introducing Amazon EC2 Fleet page.\nDon\u0026rsquo;t worry not all of this is clear to you. The important thing to know is that this CloudFormation template will create five instances (or virtual servers) three running Amazon Linux 2, and two running Windows Server.\n  1. Get to know CloudFormation Regardless of if you are using your own account an one supplied by AWS, the resources above are created using AWS CloudFormation. CloudFormation is an important service as it provides a way to rapidly and consistently deploy AWS resources. This is at the heart of the concept of infrastructure as code. Take the time (3:01) to watch the short video describing CloudFormation if you haven\u0026rsquo;t already seen in.\n  You and also visit the AWS CloudFormation Page to learn more.\nThe following steps are required only if you are completing this workshop in your own AWS account. If you are using an AWS account which has been provided as part of a AWS Event (Event Engine) then the environment will be set up for you.\n 3. Create a CloudFormation stack This workshop is currently set-up to run in the following regions;\nus-east-1, us-east-2, us-west-1, ap-southeast-1, ap-southeast-2, eu-west-1, sa-east-1.\n Select Services on the menu bar at the top of the AWS Console and select CloudFormation under Management \u0026amp; Governance.\nFirst, make sure you\u0026rsquo;re in the right region.\n  Then click Create stack \nUnder Prerequisite - Prepare template select Template is ready to tell CloudFormation that you have a pre-prepared CloudFormation script.\nUnder Specify template select Amazon S3 URL and paste in the Amazon S3 URL for the CloudFormation object URL\nTODO: Provide Object URL - detective-controls-patching.yaml\n  Click Next \n4. Specify stack details CloudFormation will now ask you to enter a StackName, enter something like patching-workshop.\n  Click Next \nThere is no need to change anything on the Configure stack options page, scroll to the bottom and click Next .\nScroll to the bottom of the final page, under Capabilities you will see a message box telling you that the CloudFormation template will create Identity and Access Management (IAM) resources, this is just a warning to ensure you don\u0026rsquo;t inadvertently compromise your security by creating unwanted permissions.\n  Select the acknowledgement check box and click Create stack . 5. Check progress You will be taken bck to the CloudFormation screen for the stack you are creating. Here you can follow the progress of the creation of resources.\n  When the stack has been deployed you will see CREATE_COMPLETE on the left-hand panel. Click the refresh button to see the events.\nYou have successfully created the resources you need to complete this workshop. you should progress to the next step.\n"},{"uri":"/6-patching-controls-ssm/3-current-os.html","title":"Determine the current OS versions","tags":["AWS Systems Manager","Detective Controls"],"description":"","content":"In this step we will use AWS Systems Manager - Inventory to determine the operating systems versions how many instances are running each operating systems.\n1. Go to AWS Systems Manager - Inventory Select Systems Manager from the AWS Console.\nIn the navigation pane, choose Inventory.\nIf you can only see the AWS Systems Manager home page, you may need to click on the menu icon (☰) in the top left to open the navigation pane, and then choose Inventory.\n  You will see something similar to the below.\n  2. Ensure Inventory is enabled Hovering your mouse over the graph to display the counts of instances. You may see that not all five of the instances in the environment have inventory enabled. i.e. instead of a fully green donut graph, you see part or fully red donut.\n  If this is the case, enable the inventory and wait for this process to complete. You may need to wait for a few minutes while the inventory is completed. Reload the page after a few minutes to check progress.\nIf you scroll down the inventory page you will see summaries of the operating system versions and other software running on the instances.\n  Hovering your mouse over the bars on the charts will display the total counts.\n"},{"uri":"/3-aws-compliance-reports/3-interpret-results.html","title":"Interpreting the results","tags":[],"description":"","content":"Now that you have a few minutes to review have the report, lets take a deeper look. First, we need to understand what the report covers, the overall conclusion and, the detailed results.\n1. Completeness check Under the Scope heading of SECTION II – Independent Service Auditor’s Assurance Report there is a list of the AWS Services, and Global Infrastructure Locations which are considered in scope.\n2. Auditor opinion Review the Opinion in SECTION II – Independent Service Auditor’s Assurance Report which provides the Auditor opinion of the report.\n3. The detailed results SECTION IV – Description of Criteria, AWS Controls, Tests and Results of Tests contains tables mapping between the Auditors Control Criteria and AWS Control Activities (AWSCA) and for each the results of the tests.\nCongratulations! you have accessed AWS Artifact, downloaded a SOC 2 Type 2 Report, and reviewed the results to confirm the controls that AWS operates on your behalf.\n "},{"uri":"/4-detective-controls-config/3-config-setup.html","title":"Enable AWS Config","tags":[],"description":"","content":"First, we need to enable AWS Config and begin tracking the configuration of the resources.\n1. Login to your AWS Account If you haven\u0026rsquo;t already logon to your AWS account, do so now. If you don\u0026rsquo;t yet have an account go to Start the Workshop\n2. Access the AWS Config Console Select the Services dropdown on the menu bar at the top left. Select Config under Management \u0026amp; Governance, or search for Config using Find Services.\nIf this is your first time using AWS Config Select Get started  from the AWS Config intro screen.\n  If you’ve already used AWS Config you will start at the AWS Config Dashboard, select Settings on the menu to the left.\n3. Select resource types to record On the Settings page, under Resource types to record, ensure Record all resources supported in this region checkbox is selected and you may also check the checkbox for Include global resources, but it\u0026rsquo;s not required for this workshop.\n  Checking these two boxes means that AWS Config will record configuration changes for all supported resources all resources as well as configuration changes for AWS Identity and Access Management (IAM) resources which are global resources. Global resources are not tied to an individual region and can be used in all regions.\n4. Create Amazon S3 bucket for configuration history AWS Config uses an S3 Bucket to store configuration history and configuration snapshot files. Under Amazon S3 bucket, select Create a bucket to have the Amazon S3 bucket created automatically.\n5. AWS Config role There is no need to change any settings in this section, Create AWS Config service-linked role should already be selected.\nIt is worth taking a moment to understand that for AWS Config to access other services like S3 we need to grant it permissions. This step creates a role for Config that grants it access to the S3 buckets we set-up in the steps above.\n Click Next .\n"},{"uri":"/7-preventative-controls-codepipeline/3-create-environments.html","title":"Create environments","tags":[],"description":"","content":" This step is required only if you are completing this workshop in your own AWS account. If you are using an AWS account which has been provided as part of a AWS Event (Event Engine) then the environment will be set up for you.\n 1. Get to know CloudFormation Take the time (3:01) to watch the short video describing CloudFormation if you haven\u0026rsquo;t already seen in.\n  You and also visit the AWS CloudFormation Page to learn more.\n2. Open CloudFormation in the AWS Console Select Services on the menu bar at the top of the AWS Console and select CloudFormation under Management \u0026amp; Governance.\n3. Create a CloudFormation stack Make sure you\u0026rsquo;re in the right region.\n  Click Create stack \nUnder Prerequisite - Prepare template select Template is ready to tell CloudFormation that you have a pre-prepared CloudFormation script.\nUnder Specify template select Amazon S3 URL and paste in the Amazon S3 URL for the CloudFormation object URL.\nTODO: Provide Object URL\n  Click Next \n4. Specify stack details CloudFormation will now ask you to enter a StackName, enter something like pipeline-workshop.\n  Click Next \nThere is no need to change anything on the next page, scroll to the bottom and click Next .\nScroll to the bottom of the final page and click Create stack .\n5. Check progress   When the stack has been deployed you will see CREATE_COMPLETE on the left-hand panel. Click the refresh button to see the events.\n "},{"uri":"/7-preventative-controls-codepipeline/4-failed-step.html","title":"Identify failed resource","tags":[],"description":"","content":"We now have environment described in the introduction including a functioning CodePipeline. The set-up has also copied the CloudFormation script we are checking into the the S3 bucket monitored by CodePipeline. Codepipeline automatically, picks up this CloudFormation script and passes it as an input into a CodePipeline execution.\nThe first task is to review this first execution of the CodePipeline.\n1. Go to the Pipeline console From the AWS Console go to AWS CodePipeline.\nCodePipeline is one of a set of services which make up Developer Tools. These services are brought together to make it easy to navigate back and forth. In this workshop you will be using the CodePipeline and CodeBuild.\nClick on \u0026ldquo;anycompany-infra-pipeline\u0026rdquo; which was created as part of the initial environment set-up.\n  2. Review the stages The Pipeline has three stages;\n Source - monitor the S3 Bucket for any changes to the file cfn-template.yaml.zip and when detected pass the changes to the Build stage. Note that in a production environment you would use a code repository tool rather than S3, but in this case we will keep it simple. Build - take the cfn-template.yaml.zip file from the source stage, unzips it, runs it through the CloudFormation checks, produce a report, and if the CloudFormation template passes all the checks, then moves the file on to the Deploy stage. Deploy - accept the script from the Build stage and passes it to AWS CloudFormation to deploy into our environment.    You can see that the pipeline has failed at the Build stage, which in this case means that the cfn-template.yaml file has not passed the all the checks.\n3. Determine what failed Click on the Details link on the Deploy_CloudFormation action in the Deploy stage.\n  The Action execution failed error message pop-up will be displayed with a link to the execution details. Click the link.\n -- You will see the most recent CodeBuild execution.\n  This screen provides you with the details of the build. Take a few minutes to explore each of the tabs.\n Build Log: A detailed log of the build steps executed. Phase Details: The phases of the build. Reports: The report generated by the build. Environment Variables: Provides a list of the variables provided into the build project - in this case there are none. Build Details: This shows the configuration of the build and commands executed.  Click on the Reports Tab, this will show you a list of the reports, one for each build stage run - in this case there will only be one unless you have run the Pipeline more than one in this lab. Click on the most recent top build report. This will take you to the results of the build.\nNote only failed test cases are shown on the report.\nThe three rules that fail are:\n W51 - S3 bucket should likely have a bucket policy W35 - S3 Bucket should have access logging configured W41 - S3 Bucket should have encryption option set  You can also see that the Prefix of the S3 Bucket that fails is S3BucketOperations.\n"},{"uri":"/3-aws-compliance-reports/4-shared-responsibility.html","title":"Deep Dive: Shared Responsibility","tags":[],"description":"","content":"AWS groups services into three main categories: infrastructure, container, and abstracted. Each category comes with a slightly different security ownership model based on how customers interact and access the functionality. Customer responsibility is determined by the AWS Cloud services that a customer selects. This determines the amount of configuration work the customer must perform as part of their security responsibilities.\n  Infrastructure Services Services such as Amazon Elastic Compute Cloud (Amazon EC2) and Amazon Virtual Private Cloud (Amazon VPC) are categorized as Infrastructure Services and, as such, require the customer to perform the necessary security configuration and management tasks. If a customer deploys an Amazon EC2 instance, they are responsible for management of the guest operating system (including updates and security patches), any application software or utilities installed by the customer on the instances, and the configuration of the AWS-provided firewall (called a security group) on each instance.\nContainer Services This is not to be mistaken with container technologies like Docker or Kubernetes.\n Services in this category typically run separately on Amazon EC2 or other infrastructure instances, but sometimes customers are not required to manage the operating system or the platform layer. AWS provides a managed service for these application “containers”. Customers are responsible for setting up and managing network controls, such as firewall rules, and for managing platform-level identity and access management separately from IAM. Examples of container services include Amazon Relational Database Services (Amazon RDS), Amazon Elastic Map Reduce (Amazon EMR) and AWS Elastic Beanstalk.\nAbstracted Services This category includes high-level storage, database, and messaging services, such as Amazon Simple Storage Service (Amazon S3), Amazon Glacier, Amazon DynamoDB, Amazon Simple Queuing Service (Amazon SQS), and Amazon Simple Email Service (Amazon SES). These services abstract the platform or management layer on which the customers can build and operate cloud applications. The customers access the endpoints of these abstracted services using AWS APIs, and AWS manages the underlying service components or the operating system on which they reside.\nTake advantage of AWS controls As every customer is deployed differently in AWS, customers can take advantage of shifting management of certain IT controls to AWS which results in a (new) distributed control environment. Customers can then use the AWS control and compliance documentation available to them to perform their control evaluation and verification procedures as required.\nMore information and examples refer to the AWS Security Best Practices Whitepaper.\nAWS also publishes security blogs related to best practices that covers best practices around using AWS services.\n"},{"uri":"/6-patching-controls-ssm/4-set-patch-baselines.html","title":"Set patch baselines","tags":["AWS Systems Manager","Detective Controls"],"description":"","content":"Now that we have enabled inventory and gained insight into the environment, let\u0026rsquo;s start on patching Patch Manager.\nTo understand how Patch Manager works it important to understand two key concepts; patch groups and patch baselines.\n Patch baseline - a set of patches and software versions which together represent the patch standard or baseline. This baseline will become obsolete when new patches are released. Patch group - a set of instances (virtual servers) that are to be patched to the same standard. This only changes when new instances are introduced or removed from the group.    Patch Baseline Patch Manager has predefined patch baselines for each operating system it supports, or if you have specific requirements you can create your own. In this workshop we will use the predefined patch baselines.\nPatch Group Patch groups are defined buy a specific Tag associated with each instance in the Patch Group. Tags are just labels and values associated with AWS resources. to learn more about tags see Tagging AWS Resources. The EC2 instances provided for this challenge have already been grouped into two Patch Groups: linux2-app-patch-group and windows-app-patch-group.\n1. Open Patch Manager - Patch Baselines Your first task is to tell Patch Manager which Patch Baselines to use for each of the two Patch Groups. To set a patch baseline open the AWS Systems Manager console and in the navigation pane, choose Patch Manager.\nYou should be taken directly to this Patch baselines screen, however if you see the AWS Patch Manager landing page, click View predefined patch baselines.\n  On the patch baselines page you will see the standard patch baseline for each of the supported operating systems. For some operating systems you may more than one entry - but there will only be one marked as Yes in the Default baseline column.\n  Select the default patch baseline for Windows - make sure it is the Default, and then under Actions select Modify patch groups\nFrom here add the Patch group windows-app-patch-group.\n  Repeat this process for the linux2-app-patch-group - Amazon Linux 2.\n"},{"uri":"/4-detective-controls-config/4-add-rules.html","title":"Add Managed Config Rules","tags":[],"description":"","content":"1. Select Config Rules In this lab we will use three of the AWS Managed Config Rules that relate to S3.\nThe rules we will use are;\n s3-bucket-server-side-encryption-enabled\nChecks that your Amazon S3 bucket either has Amazon S3 default encryption enabled or that the S3 bucket policy explicitly denies put-object requests without server side encryption that uses AES-256 or AWS Key Management Service. s3-bucket-public-write-prohibited\nChecks that your Amazon S3 buckets do not allow public write access. s3-bucket-public-read-prohibited\nChecks that your Amazon S3 buckets do not allow public read access.  Take a few minutes to review the page for each of these rules and to consider which controls these rules relate to such as access control and data encryption.\nThe easiest way to find these rules is to enter s3-bucket into the filter field.\n  Select the three rules listed above and click Next \n2. Review Config Rules   Click Confirm \n"},{"uri":"/6-patching-controls-ssm/5-review-compliance.html","title":"Review compliance","tags":["AWS Systems Manager","Detective Controls"],"description":"","content":"Now that you have set the Patch Baseline for your two Patch Groups (windows-app-patch-group and linux2-app-patch-group) you have can use Patch Manager to compare the instances in the Patch Groups against the standards defined. Where the patch levels on the instances in the Patch Groups do not meet the Patch Baseline they will be marked as not compliant.\n1. Run Patch Manager Scan From Patch Manager select Patch Now\n  Click Patch now \n2. Scan Instances Run Patch Manager to Scan only.\n  You will see the progress tracked on screen, wait for a couple on minutes for the scan to complete.\n2. Review Compliance Use AWS Systems Manager - Compliance to view the compliant and non-compliant resource.\nGo to the Compliance page in Systems Manager, you will see a screen similar to the below. However the number of compliant and non-compliant instances my vary.\n "},{"uri":"/4-detective-controls-config/5-dashboard.html","title":"Review Dashboard","tags":[],"description":"","content":"1. Explore the Dashboard Take a few minutes to explore the information on the Dashboard. This will provide you with insight into the three Managed Config Rules and the effectiveness of the associated detective controls.\nOn the right, in the Compliance status window you can see that there are a total of 3 rules - 1 Noncompliant and 2 Compliant, you can also see that there are 2 resource which are noncompliant.\nThe Noncompliant rules by noncompliant resource count lists noncompliant rules listed by the number of non-compliant resources. In this case there is only one noncompliant rule and two noncompliant resources.\nTo the left is the Resource Inventory, this lists the resources in the account. the number you see may differ from the image shown, it al depends upon what is deployed into the account.\n  2. Review noncompliant rule(s) In the Noncompliant rules window select S3-bucket-server-side-encryption-enabled. This will take you to a screen displaying the rule details and the Resources in scope.\nYou will see that we have created some example S3 buckets for business units of AnyCompany.\n  3. Drill into noncompliant resource Click on one of the bucket names to display the details of the noncompliant bucket.\n  "},{"uri":"/7-preventative-controls-codepipeline/5-review-checks.html","title":"Review checks","tags":[],"description":"","content":"From your perspective as a Risk and Assurance Manager, knowing the rules that have been applied and passed is just as important as understanding those that have failed. You need to be able to map these rules back to the AnyCompany controls framework to understand which controls are automated through these rules - this is at the heart of the concepts; \u0026ldquo;controls as code\u0026rdquo; and \u0026ldquo;automated assurance\u0026rdquo;.\nhead back to the build page\n  to make it easier to review the checks that have been run we have outputted them in the Log.\n"},{"uri":"/6-patching-controls-ssm/6-maintenance-window.html","title":"Create a maintenance window","tags":["AWS Systems Manager","Detective Controls"],"description":"","content":"AWS Systems Manager Maintenance Windows let you define a schedule for when to perform potentially disruptive actions on your instances such as patching an operating system, updating drivers, or installing software or patches. These may require Systems Manager to perform a restart so these actions should be scheduled appropriately.\nCreate a maintenance window which you will use to patch the Windows and Amazon Linux 2 instances. The details of the maintenance window details such as time, duration, frequency are all up to you.\nTo create a maintenance window goto Maintenance Windows on the Systems Manager navigation pane.\nYou can then set the Name and Description of your maintenance window to whatever you like, noting the valid characters.\nTo set the Schedule us the Cron schedule builder, but you can set schedule details as you wish.\n  Once you satisfied with your maintenance window click Save changes \n"},{"uri":"/4-detective-controls-config/6-rule-scope.html","title":"Rule Scope","tags":[],"description":"","content":"Controls should be considered from both a design and operating effectiveness perspective. As you think about your assurance program you will want to consider the frequency and breath of your assurance. With AWS Config rules you can control the frequency and scope of application of the Managed Config Rule.\nFrequency of Operation AWS Config compares your resources to the conditions of the rule. After this initial evaluation, AWS Config continues to run evaluations each time one is triggered. The evaluation triggers are defined as part of the rule, and they can include one or both of the following types:\n Configuration changes – AWS Config triggers the evaluation when any resource that matches the rule\u0026rsquo;s scope changes in configuration. The evaluation runs after AWS Config sends a configuration item change notification. Periodic – AWS Config runs evaluations for the rule at a frequency that you choose (for example, every 24 hours).  Scope of Application The rules can be applied based on;\n Type of resources - When any resource that matches the specified type, or the type plus identifier, is created, changed, or deleted - in this case we applied the rules to all S3 buckets in the account. Tags - When any resource with the specified tag is created, changed, or deleted. A tag is a simple label consisting of a customer-defined key and an optional value that can make it easier to manage, search for, and filter resources. Tags are an important part of effective operations and governance and can be used in many ways including for the classification of data. The Tagging Best Practice Whitepaper is a great resource to find out more. All changes - When any resource recorded by AWS Config is created, changed, or deleted.  To review these settings go to the Rules page by selecting Rules on the menu bar to the left and click on the s3-bucket-public-write-prohibited rule name. This will take you to the rules page.\n  Change the value of the Compliance status dropdown box to see the Compliant resources.\nNote the Trigger type will show if the rule is triggered by configuration changes and the frequency if it is periodic. The resource types shows the scope of application as discussed above - in this case the rule applies to all S3 Buckets in the account.\n "},{"uri":"/7-preventative-controls-codepipeline/6-correct-cfn.html","title":"Review code pipeline","tags":[],"description":"","content":"Go to Services and select CodePipeline under Developer Tools\nOkay, time to get your hands dirty. The Cloud Engineering Team have set you a challenge - correct the CloudFormation Template so it passes all the rules and deploys. Recall that it is the S3 bucket with the prefix S3BucketOperations that has failed and the rules that failed are: W35 - S3 Bucket should have access logging configured, W41 - S3 Bucket should have encryption option set, and W51 - S3 bucket should likely have a bucket policy.\nIn the S3 Bucket with the name prefix \u0026ldquo;anycompany-cfn-templates-\u0026rdquo;, is folder called \u0026ldquo;preventative-controls-pipeline\u0026rdquo; and within that is the cfn-template.yaml.zip which contains the CloudFormation Template you will need to edit.\nNOTE: To help you along the way the cfn-template.yaml file includes the script for the Payroll bucket which works and plenty of comments. These begin with the \u0026lsquo;#\u0026rsquo; character and anything to the right of it is a comment. The other thing you MUST take note of is indentation. This file uses two spaces for each indent level and you must follow the same indentation as the payroll bucket.\nTo edit the CloudFormation Template you don\u0026rsquo;t need anything fancy, however there are plenty of free code editor you could use. They do make life easier by providing line numbers, indentation guides and formatting your code with colour but TextEdit on Mac or Notepad on Windows will do.\nYour Task Update the cfn-template.yaml, zip it up, and replace cfn-template.yaml.zip in the S3 bucket. Once this is done the pipeline will run automatically and if you new template is correct the S3 buckets will deploy.\n  Download the CloudFormation File\nGo to the S3 bucket with the name that starts with \u0026ldquo;anycompany-cfn-templates\u0026rdquo;, open the folder called \u0026ldquo;preventative-controls-pipeline\u0026rdquo; and within that is the cfn-template.yaml.zip which contains the CloudFormation Template you will need to edit. Download this zip file to a working directory on your computer.\n  Edit the CloudFormation File\nUnzip the file, open it with an editor and update the file to look like the below. Basically all you need to do is a copy and paste of properties section from the S3BucketPayroll bucket to the S3BucketOperations bucket and then change references to the bucket name in a few places.\nMake sure you save the changes to the cfn-template.yaml file, don\u0026rsquo;t change the name.\n  AWSTemplateFormatVersion: 2010-09-09 Description: Risk Jam - Preventative Controls Parameters: S3AccessLoggingBucket: Type: String Resources: ############################################################################ # Payroll Department S3 Bucket # # This bucket configuration includes encryption, access logging,  # and a bucket policy which restricts access. ############################################################################ PayrollRole: # Name of the role - should match the business unit Type: AWS::IAM::Role Properties: AssumeRolePolicyDocument: Version: \u0026#39;2012-10-17\u0026#39; Statement: - Effect: Allow Principal: AWS: !Sub arn:aws:iam::${AWS::AccountId}:root Action: - sts:AssumeRole PayrollBucketPolicy: # Name of the bucket policy - should match the business unit Type: AWS::S3::BucketPolicy Properties: Bucket: Ref: S3BucketPayroll # MUST MATCH BUCKET NAME PolicyDocument: Statement: - Action: - s3:GetObject Effect: Allow Resource: !Sub \u0026#39;${S3BucketPayroll.Arn}/*\u0026#39; # MUST MATCH BUCKET NAME - DON\u0026#39;T CHANGE ANYTHING ELSE  Principal: AWS: - !GetAtt PayrollRole.Arn # MUST MATCH BUCKET NAME - DON\u0026#39;T CHANGE ANYTHING ELSE S3BucketPayroll: # Name of the S3 bucket resource - should match the business unit Type: AWS::S3::Bucket Properties: BucketEncryption: ServerSideEncryptionConfiguration: - ServerSideEncryptionByDefault: SSEAlgorithm: AES256 LoggingConfiguration: DestinationBucketName: !Ref S3AccessLoggingBucket LogFilePrefix: payroll-logs # update to match business unit ############################################################################ # Operations Department S3 Bucket # # This bucket configuration includes encryption, access logging,  # and a bucket policy which restricts access. ############################################################################ OperationsRole: # Name of the role - should match the business unit Type: AWS::IAM::Role Properties: AssumeRolePolicyDocument: Version: \u0026#39;2012-10-17\u0026#39; Statement: - Effect: Allow Principal: AWS: !Sub arn:aws:iam::${AWS::AccountId}:root Action: - sts:AssumeRole OperationsBucketPolicy: # Name of the bucket policy - should match the business unit Type: AWS::S3::BucketPolicy Properties: Bucket: Ref: S3BucketOperations # MUST MATCH BUCKET NAME PolicyDocument: Statement: - Action: - s3:GetObject Effect: Allow Resource: !Sub \u0026#39;${S3BucketOperations.Arn}/*\u0026#39; # MUST MATCH BUCKET NAME - DON\u0026#39;T CHANGE ANYTHING ELSE  Principal: AWS: - !GetAtt OperationsRole.Arn # MUST MATCH BUCKET NAME - DON\u0026#39;T CHANGE ANYTHING ELSE S3BucketOperations: # This is the name of the bucket resource Type: AWS::S3::Bucket Properties: BucketEncryption: ServerSideEncryptionConfiguration: - ServerSideEncryptionByDefault: SSEAlgorithm: AES256 LoggingConfiguration: DestinationBucketName: !Ref S3AccessLoggingBucket LogFilePrefix: operations-logs # update to match business unit  Zip the file\nZip up the cfn-template.yaml file into a new version of cfn-template.yaml.zip - make sure you use this exact file name.\n  Upload the file to S3\nUpload the new cfn-template.yaml.zip file to the S3 \u0026ldquo;anycompany-cfn-templates\u0026rdquo; bucket, replacing the old file. Make sure that the file is in the \u0026ldquo;preventative-controls-pipeline\u0026rdquo; folder and replaces the old version.\n  Check the Pipeline\nYou can head back to the pipeline to see it pick up the new CloudFormation file and run it through the pipeline.\n  Note that under \u0026ldquo;Most recent execution\u0026rdquo; nothing is shown. This is because we haven\u0026rsquo;t yet provided a change for our code pipeline to process.\nClick on the name of your pipeline to see the stages.\nAgain, note that the first stage has Failed, but don\u0026rsquo;t worry it is only because the code pipeline tried to run when it is first provisioned but there was no file for it to process. We\u0026rsquo;ll provide a file soon.\nYou should have also received and email titled \u0026ldquo;AWS Notifications\u0026rdquo;, this was generated by the pipeline and will require you to confirm your subscription, which you should do now.\nCongratulations, you now have a functioning code pipeline! Now lets try it out.\n"},{"uri":"/6-patching-controls-ssm/7-patch-manager-config.html","title":"Create Patch Manager configuration","tags":[],"description":"","content":"Now that you have setup the patch baselines and the maintenance window there is one final task to configure Patch Manager to automatically apply patches as per your schedule.\nConfigure Patch Manager to apply patched to both of your patch groups during the maintenance window you set up in the previous task.\nFrom the Patch Manager console select Configure patching \n  Complete the Patch manager configuration by selecting the two patch groups, the maintenance schedule you created earlier, and selecting Scan and install, then click Configure patching.\n "},{"uri":"/4-detective-controls-config/7-remediate.html","title":"Auto Remediate","tags":[],"description":"","content":"Now we will introduce an automated corrective remediation to the s3-bucket-server-side-encryption-enabled config rule. This means that AWS Config will automatically apply server side encryption to any bucket in the account found that doesn\u0026rsquo;t have it turned on. This will move the rule from being detective to corrective. In a production environment you want to carefully choose the actions you automate and the resources that these actions apply to, but automation is a powerful way to move to a more proactive risk and controls management stance.\n1. Get the AutomationAssumeRole ARN To enable the auto remediation action you will need to provide Config with the permission to perform the auto remediation actions. The required role and permissions were created as part of the lab set-up but we need to apply them to the Manage Config Rule. To identify the Role we\u0026rsquo;ll need the Role reference ID or Amazon Resource Name (ARN). The ARN is used to uniquely identify the role.\nTo find the Role ARN go to Roles in AWS Identity and Access Management (IAM).\nUse the Services dropdown to navigate to IAM and select Roles. From this screen you will see a list of existing roles and up the top you should see AutomationServiceRole.\n  Click on the name of the role to see a summary of the role. At the top of this screen you will see Role ARN. Copy the ARN (you can do so easily by clicking the copy icon to the right). This is the string you will need to paste into the AutomatedAssumeRole back in Config.\n  While you\u0026rsquo;re here, you can see that the permissions of this role are defined by the policy \u0026ldquo;ConfigAutoRemediationPolicy\u0026rdquo;. Which makes sense given what we are tying to do. If you have time you gan dig in a bit deeper to see the exact permissions this policy grants.\n2. Select the s3-bucket-server-side-encryption-enabled rule Now that you have the Role ARN it\u0026rsquo;s time to head back to AWS Config. From the AWS Config Console select Rules on the navigation menu on the left.\n  From here you will see the three rules you implemented, and that the Remediation action for each is \u0026ldquo;Not set\u0026rdquo;. Click on s3-bucket-server-side-encryption-enabled.\n3. Configure Auto remediation From the rules page, click on the Actions button at the top right, and select Manage remediation.\n  Complete the remediation section of the form as per below.\n  Ensure Automatic remediation is selected.\n  Set Remediation action details to AWS-EnablesS3BucketEncryption.\n  The Resource ID Parameter must be set to BucketName. This tells Config that the Bucket Name will be variable.\n  The three parameters are;\n BucketName The name of the bucket to apply the auto remediation to. You will should see this is greyed out because you selected BucketName as the Resource ID Parameter. You do not want to specify a particular bucket because we want the remediation to be applied to all buckets. SSEAlgorithm The encryption algorithm, this must be set to AES256. AutoAssumeRole This is the role ARN that you copies in step 1. This allows the auto remediation to happen.    Click Save changes \nYou will then be taken back to the Rules page where you will see the Remediation action has been set but the Rule is still showing noncompliant resource(s). It will take around 5-10 minutes for the remediation actions to run and for the config status to be refreshed. Move on to the next step and we\u0026rsquo;ll come back and check later.\n"},{"uri":"/7-preventative-controls-codepipeline/7-check-change.html","title":"Deploy change","tags":[],"description":"","content":"1. Unzip the change file\nIn Step 1 - Get Source Files we downloaded the source files. Go to the directory which contains these files and unzip the codepipe-single-sg.zip file, it contains three files;\n single_security_group_one_cidr_ingress.json\nThis file is the main CloudFormation script for our change containing the configuration for the security group that we will deploy first into the Testing Environment, then into the Production Environment. test-stack-configuration.json\nThis file includes the name of the VPC used for our testing environment. prod-stack-configuration.json This file includes the name of the VPC used for the production environment.  2. Review the main CloudFormation template\nFirst, lets have a look at the single_security_group_one_cidr_ingress.json file. This file creates a security group - a security group acts as a virtual firewall for your instance to control inbound and outbound traffic.\nWe\u0026rsquo;re not going to get into the format or structure of the file in this lab but there are a few lines that we will focus on. Where the file provides the details for the SecurityGroupIngress you can see that the CidrIp range is set to 0.0.0.0/0 and the FromPort is 22. This means that the security group is open to remote connection from any ip address - which we do not want - but lets leave it as is for now.\n{ \u0026#34;AWSTemplateFormatVersion\u0026#34;: \u0026#34;2010-09-09\u0026#34;, \u0026#34;Description\u0026#34;: \u0026#34;Sample, test only template that creates a sample Security Group allowing all SSH traffic inbound from 0.0.0.0/0 . This template is for testing purposes only.\u0026#34;, \u0026#34;Parameters\u0026#34;: { \u0026#34;VPCName\u0026#34;: { \u0026#34;Description\u0026#34;: \u0026#34;ID of the existing VPC.\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;AWS::EC2::VPC::Id\u0026#34; } }, \u0026#34;Resources\u0026#34;: { \u0026#34;sg\u0026#34;: { \u0026#34;Type\u0026#34; : \u0026#34;AWS::EC2::SecurityGroup\u0026#34;, \u0026#34;Properties\u0026#34; : { \u0026#34;GroupDescription\u0026#34; : \u0026#34;SSH Security Group\u0026#34;, \u0026#34;SecurityGroupIngress\u0026#34; : { \u0026#34;CidrIp\u0026#34;: \u0026#34;0.0.0.0/0\u0026#34;, \u0026#34;FromPort\u0026#34;: 22, \u0026#34;ToPort\u0026#34;: 22, \u0026#34;IpProtocol\u0026#34;: \u0026#34;tcp\u0026#34; }, \u0026#34;Tags\u0026#34;: [ {\u0026#34;Key\u0026#34;: \u0026#34;Name\u0026#34;, \u0026#34;Value\u0026#34;: \u0026#34;Immersion-Day-Demo\u0026#34;}, {\u0026#34;Key\u0026#34;: \u0026#34;LOB\u0026#34;, \u0026#34;Value\u0026#34;: \u0026#34;Risk-\u0026amp;-Compliance\u0026#34;} ], \u0026#34;VpcId\u0026#34; : { \u0026#34;Ref\u0026#34; : \u0026#34;VPCName\u0026#34; }} } } } 3. Update the CloudFormation templates\nNext we will update the two configuration files with the names of the VPCs we noted in Step 5 - Check CloudFormation.\n test-stack-configuration.json\nUpdate the VPCName to the name of your test environment VPC. prod-stack-configuration.json Update the VPCName to the name of your production environment VPC.  Your test-stack-configuration.json and prod-stack-configuration.json files should look like this but with your vpc as the value for VPCName.\n{ \u0026#34;Parameters\u0026#34; : { \u0026#34;VPCName\u0026#34; : \u0026#34;vpc-cd0dd4a9\u0026#34; } } Don\u0026rsquo;t forget to save your changes.\n4. Create new codepipe-single-sg.zip file\nZip up the three files into a new codepipe-single-sg.zip\n5. Upload the CloudFormation script to the code repository bucket\nUpload the new codepipe-single-sg.zip file into the S3 bucket you created in Step 2 - Create Repo Bucket.\n6. Go to the Pipeline\nThe AWS CodePipeline will detect that the file is in the S3 bucket and will run automatically, or you can click on the pipeline name and click Release change \nYou can see that the StaticCodeAnalysis stage has failed, to see why click the Details link just below Failed.\nYou can see the execution failed due to \u0026ldquo;Failed filters \u0026lsquo;IngressOpenToWorld\u0026rsquo;, \u0026lsquo;SSHOpenToWorld\u0026rsquo;\u0026quot;. This is exactly the issue we saw when we reviewed the code earlier. To dive even deeper click Link to execution details. this will take you to Amazon CloudWatch.\n"},{"uri":"/6-patching-controls-ssm/8-patch-now.html","title":"Patch Now","tags":[],"description":"","content":"Your final step is to patch the instances in your environment now so you can see the results in the Compliance\nFrom the Patch Manager console select Configure patching \n  Click Patch now \nUpdate the options as per below, including Scan and Install and Reboot if needed.\n  Click Patch now \nThe patch progress screen will be displayed.\n  It may take awhile to update as Patch Manager applies all the required operating system and agent updates. This screen will automatically update as the patching operation progresses.\nCongratulations! You have successfully completed this workshop.\n "},{"uri":"/4-detective-controls-config/8-timelines.html","title":"Explore Timelines","tags":[],"description":"","content":"1. Return to Config Select Resources in the navigation pane and type \u0026ldquo;Bucket\u0026rdquo; in to the Resource type drop down - make sure you select the tick-box and Click Look up.\n  2. Select a bucket Click on the bucket name that starts with \u0026ldquo;config-bucket\u0026rdquo;\n  2. Access Resource timelines From the Resource screen click Resource Timeline \n  3. Review Resource configuration timeline You should now see the Configuration timeline take some time to explore the events on the timeline.\nYou should now see that the auto remediation that you setup has run and that the bucket is compliant.\nYou have successfully completed this workshop, if you are using your own AWS Account you should now complete the clean-up step.\n "},{"uri":"/4-detective-controls-config/9-clean-up.html","title":"Clean up","tags":[],"description":"","content":" If you are using your own AWS account this step is recommended to avoid being billed for resources you are not using. If you are using an AWS account which has been provided as part of a AWS Event (Event Engine) then the environment will be cleaned up for you.\n 1. Turn off Config Go to the settings page and under Recording is on click the Turn off  button.\nAlso take note of the name of the S3 bucket used by Config for logs.\n  A pop-up will ask you to confirm turning off recording.\n  Confirm by clicking Continue .\nEdit  - Scroll to the bottom of the page and click Delete rule  - On the confirmation pop-up ![Config Rules](config-delete-rule-confirm.png) Click Delete  to confirm deletion of the rule. -- Delete . ![Confirm Delete Subscription](sns-delete-sub-confirm.png) On the confirmation pop-up click Delete . Now select Topics from the menu bar. ![Delete Topic](sns-delete-topic.png) Select the radio button next to the topic and click Delete . ![Confirm Delete Topic](sns-delete-topic-confirm.png) To confirm deletion click Delete . -- 2. Delete Config logging bucket Go to the S3 Console and select the bucket used by config to store logs. Select Empty .\nOnce the S3 bucket is empty then select Delete \n3. Delete CloudFormation stack Return to CloudFormation and you will see your stack listed, select it and click Delete \nCongratulations, you have now completed the lab and cleaned up the account.\n "},{"uri":"/6-patching-controls-ssm/9-clean-up.html","title":"Clean up","tags":[],"description":"","content":" If you are using your own AWS account this step is recommended to avoid being billed for resources you are not using. If you are using an AWS account which has been provided as part of a AWS Event (Event Engine) then the environment will be cleaned up for you.\n 1. Delete CloudFormation stack Return to CloudFormation and you will see the stack you created in step 2 - Set-up. If you used the suggested name it will be called patching-workshop.\nSelect it and click Delete \nCongratulations, you have now completed the lab and cleaned up the account.\n "},{"uri":"/7-preventative-controls-codepipeline/10-clean-up.html","title":"Clean up","tags":[],"description":"","content":"1. Delete S3 artifactstorebucket\nWhen we ran the CloudFormation template to create the CodePipeline, one of the resources created was an S3 bucket the CodePipeline uses to store it\u0026rsquo;s working files. We need to empty this bucket in order for the CloudFormation deletion to complete.\nSelect the checkbox beside the bucket that\u0026rsquo;s name starts with pipeline-controls-lab-stack-artifactstorebucket- and click Empty \nYou can only delete empty S3 buckets. Deletion fails for buckets that have contents.\n 2. Delete CloudFormation Stacks\nGo to Cloudformation and delete;\n Prod-SG pipeline-controls-lab-environments pipeline-controls-lab-stack  "},{"uri":"/7-preventative-controls-codepipeline.html","title":"Preventative controls with Code Pipeline","tags":[],"description":"","content":"A powerful way to implement preventative controls is to embed them into code pipelines.\nIn this workshop we demonstrate a code pipeline which takes an AWS CloudFormation template and runs checks against it, if it passes the template is then deployed into a testing environment where additional checks are run, and finally if the template meets requirements it can be deployed into the production environment.\nDuration The activity should take approximately 30 minutes to complete.\nOutcome By the completion of this lab you will have an understanding of the Shared Responsibility Model and be able to access report from AWS Artifact.\nTarget Audience This workshop has been developed specifically with the risk, compliance, and controls assurance community in mind, but anyone interested in understanding how to evidence AWS controls will benefit.\nPrerequisites Aside from and AWS Account, there are no prerequisites and no assumed AWS knowledge for this workshop.\nCosts There are no costs associated with this lab.\nSteps  Introduction   Create environments   Identify failed resource   Review checks   Review code pipeline   Deploy change   Clean up   "},{"uri":"/tags/artifact.html","title":"Artifact","tags":[],"description":"","content":""},{"uri":"/tags/aws-config.html","title":"AWS Config","tags":[],"description":"","content":""},{"uri":"/tags/aws-systems-manager.html","title":"AWS Systems Manager","tags":[],"description":"","content":""},{"uri":"/categories/beginner.html","title":"Beginner","tags":[],"description":"","content":""},{"uri":"/categories.html","title":"Categories","tags":[],"description":"","content":""},{"uri":"/tags/detective-controls.html","title":"Detective Controls","tags":[],"description":"","content":""},{"uri":"/tags/security-of-the-cloud.html","title":"Security of the Cloud","tags":[],"description":"","content":""},{"uri":"/tags.html","title":"Tags","tags":[],"description":"","content":""},{"uri":"/categories/advanced.html","title":"Advanced","tags":[],"description":"","content":""}]